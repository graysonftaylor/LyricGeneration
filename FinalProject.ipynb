{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p1cRiAOIDMG"
      },
      "source": [
        "Lyric Generation: 3 Ways (XLNET, GPT-2, LSTM with Pytorch)\n",
        "\n",
        "Collaborators: Aleks Sekulovski + Grayson Taylor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyFEcFvzYOxq",
        "outputId": "bf8f35a2-e186-412e-a747-c6e2b5ea75e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_JZIgd3HgCER",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e8d22cdb-ea2a-498b-c67f-f8f70bb671be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.8/dist-packages (1.1.4)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.8/dist-packages (from flask) (1.0.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.8/dist-packages (from flask) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.8/dist-packages (from flask) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.8/dist-packages (from flask) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2<3.0,>=2.10.1->flask) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spotipy in /usr/local/lib/python3.8/dist-packages (2.22.0)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spotipy) (1.15.0)\n",
            "Requirement already satisfied: redis>=3.5.3 in /usr/local/lib/python3.8/dist-packages (from spotipy) (4.4.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.8/dist-packages (from spotipy) (1.26.13)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.8/dist-packages (from spotipy) (2.28.1)\n",
            "Requirement already satisfied: async-timeout>=4.0.2 in /usr/local/lib/python3.8/dist-packages (from redis>=3.5.3->spotipy) (4.0.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.25.0->spotipy) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.25.0->spotipy) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.25.0->spotipy) (2022.9.24)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Using cached huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.13)\n",
            "Installing collected packages: huggingface-hub\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.7.0\n",
            "    Uninstalling huggingface-hub-0.7.0:\n",
            "      Successfully uninstalled huggingface-hub-0.7.0\n",
            "Successfully installed huggingface-hub-0.11.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.8/dist-packages (0.1.91)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gpt-2-simple in /usr/local/lib/python3.8/dist-packages (0.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gpt-2-simple) (4.64.1)\n",
            "Requirement already satisfied: toposort in /usr/local/lib/python3.8/dist-packages (from gpt-2-simple) (1.7)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from gpt-2-simple) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from gpt-2-simple) (2.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from gpt-2-simple) (1.21.6)\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.8/dist-packages (from gpt-2-simple) (2.9.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.9.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (21.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (57.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.14.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.1.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.12)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.19.6)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.9.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (14.0.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.28.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.51.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.38.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (2.15.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (0.4.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->gpt-2-simple) (1.26.13)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->gpt-2-simple) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->gpt-2-simple) (2022.9.24)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->gpt-2-simple) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow>=2.5.1->gpt-2-simple) (3.0.9)\n",
            "Tue Dec 13 22:21:28 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    51W / 400W |    632MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.13)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.7.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.11.1)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.1.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.28.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.26.13)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting huggingface-hub==0.7\n",
            "  Using cached huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from huggingface-hub==0.7) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub==0.7) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub==0.7) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub==0.7) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub==0.7) (2.28.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub==0.7) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub==0.7) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub==0.7) (2022.9.24)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub==0.7) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub==0.7) (1.26.13)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub==0.7) (2.1.1)\n",
            "Installing collected packages: huggingface-hub\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.11.1\n",
            "    Uninstalling huggingface-hub-0.11.1:\n",
            "      Successfully uninstalled huggingface-hub-0.11.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.25.1 requires huggingface-hub<1.0,>=0.10.0, but you have huggingface-hub 0.7.0 which is incompatible.\u001b[0m\n",
            "Successfully installed huggingface-hub-0.7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.8/dist-packages (0.13.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.1.29)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.28.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.0.11)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.8/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.8/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.11.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.13)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lyricsgenius in /usr/local/lib/python3.8/dist-packages (3.0.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from lyricsgenius) (4.6.3)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.8/dist-packages (from lyricsgenius) (2.28.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20.0->lyricsgenius) (1.26.13)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20.0->lyricsgenius) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20.0->lyricsgenius) (2022.9.24)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20.0->lyricsgenius) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (3.8.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp) (6.0.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp) (22.1.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.8/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from langdetect) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.8/dist-packages (0.15.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.13.0+cu116)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.21.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate) (5.4.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.4.0->accelerate) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.8/dist-packages (0.4.1)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.8/dist-packages (0.4.1)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from jax) (1.21.6)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax) (3.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/google/flax.git\n",
            "  Cloning https://github.com/google/flax.git to /tmp/pip-req-build-yyiqv59g\n",
            "  Running command git clone -q https://github.com/google/flax.git /tmp/pip-req-build-yyiqv59g\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.8/dist-packages (from flax==0.6.3) (1.21.6)\n",
            "Requirement already satisfied: jax>=0.3.16 in /usr/local/lib/python3.8/dist-packages (from flax==0.6.3) (0.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from flax==0.6.3) (3.2.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.8/dist-packages (from flax==0.6.3) (1.0.4)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.8/dist-packages (from flax==0.6.3) (0.1.4)\n",
            "Requirement already satisfied: orbax in /usr/local/lib/python3.8/dist-packages (from flax==0.6.3) (0.0.23)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.8/dist-packages (from flax==0.6.3) (0.1.28)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.8/dist-packages (from flax==0.6.3) (12.6.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from flax==0.6.3) (4.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.8/dist-packages (from flax==0.6.3) (6.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.16->flax==0.6.3) (1.7.3)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.16->flax==0.6.3) (3.3.0)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from rich>=11.1->flax==0.6.3) (0.9.1)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich>=11.1->flax==0.6.3) (2.6.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->flax==0.6.3) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->flax==0.6.3) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->flax==0.6.3) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->flax==0.6.3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->flax==0.6.3) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from optax->flax==0.6.3) (1.3.0)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.8/dist-packages (from optax->flax==0.6.3) (0.4.1)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.8/dist-packages (from optax->flax==0.6.3) (0.1.5)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax->flax==0.6.3) (0.1.7)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax->flax==0.6.3) (0.12.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.8/dist-packages (from orbax->flax==0.6.3) (3.6.4)\n",
            "Requirement already satisfied: cached_property in /usr/local/lib/python3.8/dist-packages (from orbax->flax==0.6.3) (1.5.2)\n",
            "Requirement already satisfied: etils in /usr/local/lib/python3.8/dist-packages (from orbax->flax==0.6.3) (0.9.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.8/dist-packages (from orbax->flax==0.6.3) (5.10.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib_resources->orbax->flax==0.6.3) (3.11.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax==0.6.3) (1.4.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax==0.6.3) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax==0.6.3) (57.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax==0.6.3) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax==0.6.3) (22.1.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax==0.6.3) (9.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: hf-lfs in /usr/local/lib/python3.8/dist-packages (0.0.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.8/dist-packages (1.5.6)\n"
          ]
        }
      ],
      "source": [
        "# installs\n",
        "!pip install flask\n",
        "!pip install spotipy\n",
        "!pip install transformers\n",
        "!pip install sentencepiece==0.1.91\n",
        "!pip install gpt-2-simple\n",
        "!nvidia-smi\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install huggingface-hub==0.7\n",
        "!pip install torch\n",
        "!pip install wandb\n",
        "!pip install lyricsgenius\n",
        "!pip install aiohttp\n",
        "!pip install langdetect\n",
        "!pip install accelerate\n",
        "!pip install --upgrade jax jaxlib \n",
        "!pip install --upgrade git+https://github.com/google/flax.git\n",
        "!pip install tqdm --upgrade\n",
        "!pip install hf-lfs\n",
        "!pip install nest_asyncio\n",
        "\n",
        "# imports\n",
        "from transformers import XLNetConfig, XLNetModel, XLNetLMHeadModel, XLNetTokenizer\n",
        "import torch\n",
        "import lyricsgenius\n",
        "import spotipy\n",
        "from spotipy.oauth2 import SpotifyClientCredentials\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import normalize\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from IPython.display import display, HTML, Javascript, clear_output\n",
        "import lyricsgenius\n",
        "from tqdm.notebook import tqdm as bar\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from datasets import Dataset, DatasetDict\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "import langdetect\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "import wandb\n",
        "import pathlib\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import asyncio\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import sys\n",
        "sys.setrecursionlimit(99999)\n",
        "import aiohttp\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import urllib.request\n",
        "import urllib.parse\n",
        "import json\n",
        "import flask\n",
        "import pandas as pd\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import gpt_2_simple as gpt2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as Functional\n",
        "import numpy as np\n",
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approach 1: LSTM Model (Pytorch)"
      ],
      "metadata": {
        "id": "DvrayUQ7K2xR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YHaNSl9QKK8g"
      },
      "outputs": [],
      "source": [
        "### Credits: https://www.analyticsvidhya.com/blog/2020/08/build-a-natural-language-generation-nlg-system-using-pytorch/\n",
        "\n",
        "# clean alphabet for word generation\n",
        "def clean_lyric(lyric):\n",
        "    return re.sub(\"[^a-z' ]\", \"\", lyric).replace(\"'\", \"\")\n",
        "\n",
        "# create and return list of all possible sequences\n",
        "def create_sequences(lyric, seq_len):\n",
        "    sequences = []\n",
        "    if len(lyric.split()) <= seq_len:\n",
        "        return [lyric]\n",
        "    for itr in range(seq_len, len(lyric.split())):\n",
        "        curr_seq = lyric.split()[itr - seq_len:itr + 1]\n",
        "        sequences.append(\" \".join(curr_seq))\n",
        "    return sequences\n",
        "\n",
        "# gets numbers for words in input sequence\n",
        "def get_seq_idx(sequence):\n",
        "    return [word_to_idx[word] for word in sequence.split()]\n",
        "\n",
        "# gets the next batch\n",
        "def get_next_batch(x, y, batch_size):\n",
        "    for itr in range(batch_size, x.shape[0], batch_size):\n",
        "        batch_x = x[itr - batch_size:itr, :]\n",
        "        batch_y = y[itr - batch_size:itr, :]\n",
        "        yield batch_x, batch_y\n",
        "\n",
        "# make inputs, detach hidden state, get model output, get token Ps and reshape\n",
        "# get top 3 values, select 1 and return it (along with the hidden state)\n",
        "def predict(model, token, hidden_layer):\n",
        "    x = np.array([[word_to_idx[token]]])\n",
        "    inputs = torch.from_numpy(x).type(torch.LongTensor)\n",
        "    hidden = tuple([layer.data for layer in hidden_layer])\n",
        "    out, hidden = model(inputs, hidden)\n",
        "    prob = Functional.softmax(out, dim=1).data.numpy()\n",
        "    prob = prob.reshape(prob.shape[1],)\n",
        "    top_tokens = prob.argsort()[-3:][::-1]\n",
        "    selected_index = top_tokens[0]\n",
        "    return idx_to_word[selected_index], hidden\n",
        "\n",
        "# creates the string\n",
        "def generate(model, num_words, start_text):\n",
        "    model.eval()\n",
        "    hidden = model.init_hidden(1)\n",
        "    tokens = start_text.split()\n",
        "    for token in start_text.split():\n",
        "        curr_token, hidden = predict(model, token, hidden)\n",
        "    tokens.append(curr_token)\n",
        "    for token_num in range(num_words - 1):\n",
        "        token, hidden = predict(model, tokens[-1], hidden)\n",
        "        tokens.append(token)\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# calls generate to create output string\n",
        "def get_lyric(start_text, num_words):\n",
        "    return generate(model, num_words, start_text.lower())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the meaty part (LSTM)\n",
        "class LSTM(nn.Module):\n",
        "    # initialize variables\n",
        "    def __init__(self, num_hidden, num_layers, embed_size, drop_prob, lr):\n",
        "        #print(\"Initializing model...\")\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.num_layers = num_layers\n",
        "        self.num_hidden = num_hidden\n",
        "        self.lr = lr\n",
        "        # define embedded layer, LSTM, dropout layer, andd fully connected layer\n",
        "        self.embedded = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, num_hidden, num_layers, dropout = drop_prob, batch_first = True)\n",
        "        self.dropout = nn.Dropout(drop_prob) \n",
        "        self.fc = nn.Linear(num_hidden, vocab_size)      \n",
        "    \n",
        "    # forward propagate\n",
        "    def forward(self, x, hidden_layer):      \n",
        "        # pass input through embedding layer\n",
        "        embedded = self.embedded(x)        \n",
        "        # get outputs and hidden layer from LSTM layer\n",
        "        lstm_output, hidden_layer = self.lstm(embedded, hidden_layer)        \n",
        "        # pass through a dropout layer and reshape\n",
        "        dropout_out = self.dropout(lstm_output).reshape(-1, self.num_hidden) \n",
        "        # put \"out\" through the fully-connected layer\n",
        "        out = self.fc(dropout_out)\n",
        "        # return final output and hidden layer\n",
        "        return out, hidden_layer\n",
        "    \n",
        "    # init hidden layer\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Create a weight torch using the parameters of the model\n",
        "        weight = next(self.parameters()).data\n",
        "        # initialize the hidden layer using the weight torch\n",
        "        hidden_layer = (weight.new(self.num_layers, batch_size, self.num_hidden).zero_(), weight.new(self.num_layers, batch_size, self.num_hidden).zero_())\n",
        "        # return the hidden layer\n",
        "        return hidden_layer"
      ],
      "metadata": {
        "id": "V8Rv76RqlO2I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/drive/MyDrive/adele.txt\", \"r\", encoding = \"utf8\")\n",
        "text = file.read()\n",
        "text = text.replace(\"\\n\\n\", \"\\n\")\n",
        "\n",
        "lyrics = text.lower().split(\"\\n\")\n",
        "lyrics = np.unique(lyrics)[1:].tolist()\n",
        "\n",
        "cleaned_lyrics = [clean_lyric(lyric) for lyric in lyrics]\n",
        "\n",
        "seq_size = 5\n",
        "\n",
        "# get every sequence\n",
        "raw_sequences = [create_sequences(lyric, seq_size) for lyric in cleaned_lyrics]\n",
        "\n",
        "# unique sentences only\n",
        "sequences = np.unique(np.array(sum(raw_sequences, []))).tolist()\n",
        "print(sequences)\n",
        "\n",
        "uniq_words = np.unique(np.array(\" \".join(sequences).split(\" \")))\n",
        "uniq_words_idx = np.arange(uniq_words.size)\n",
        "\n",
        "word_to_idx = dict(zip(uniq_words.tolist(), uniq_words_idx.tolist()))\n",
        "idx_to_word = dict(zip(uniq_words_idx.tolist(), uniq_words.tolist()))\n",
        "\n",
        "vocab_size = len(word_to_idx)\n",
        "\n",
        "# intialize empty lists\n",
        "x_word = []\n",
        "y_word = []\n",
        "\n",
        "# iterate through every sequence\n",
        "for seq in sequences:\n",
        "    if (len(seq.split()) != seq_size + 1):\n",
        "        continue\n",
        "    # add words to sequences\n",
        "    x_word.append(\" \".join(seq.split()[:-1]))\n",
        "    y_word.append(\" \".join(seq.split()[1:]))\n",
        "\n",
        "x_idx = np.array([get_seq_idx(word) for word in x_word])\n",
        "y_idx = np.array([get_seq_idx(word) for word in y_word])\n",
        "\n",
        "# initialize parameters for LSTM model\n",
        "num_hidden = 256\n",
        "num_layers = 4\n",
        "embed_size = 200\n",
        "drop_prob = 0.3\n",
        "lr = 0.001\n",
        "num_epochs = 30\n",
        "batch_size = 30\n",
        "\n",
        "# create LSTM model, choosing optimizer and loss function, and training model\n",
        "model = LSTM(num_hidden, num_layers, embed_size, drop_prob, lr)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # initialize hidden state\n",
        "    hidden_layer = model.init_hidden(batch_size)\n",
        "        \n",
        "    for x, y in get_next_batch(x_idx, y_idx, batch_size):            \n",
        "        # numpy -> Pytorch\n",
        "        inputs = torch.from_numpy(x).type(torch.LongTensor)\n",
        "        act = torch.from_numpy(y).type(torch.LongTensor)\n",
        "        # reformat hidden layer\n",
        "        hidden_layer = tuple([layer.data for layer in hidden_layer])\n",
        "        # get zero-accumulated gradients from the model\n",
        "        model.zero_grad()            \n",
        "        # get output\n",
        "        output, hidden = model(inputs, hidden_layer)           \n",
        "        # calculate loss from this prediction\n",
        "        loss = loss_func(output, act.view(-1))\n",
        "        # backpropagation\n",
        "        loss.backward()\n",
        "        # prevents exploding gradient problem\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "        optimizer.step()    \n",
        "\n",
        "get_lyric(\"love\", 8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "RUOzzIWYkfmj",
        "outputId": "da4d370c-cbd3-455f-e0eb-b53e9563ed78"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a fuck so hes probably just', 'a fuckin shrink sheesh i already', 'a fuckin walkin paradox no im', 'a motherfuckin goblin', 'about five seven of his bitches', 'about i start a team of', 'ace gon put that fuckin hole', 'actions speak louder than words let', 'after bowling i went home for', 'aint no v tech shit or', 'airplane that that faggot nigga bob', 'all i want fuck money diamonds', 'all your problems hes fuckin awesome', 'an overachiever so how about i', 'and danced around the house in', 'and hes not fuckin workin i', 'and im wolf that was me', 'and pick up stevie wonder to', 'and stab bruno mars in his', 'and tina still aint perm her', 'and wont stop until the cops', 'any bloggin faggot hipster with a', 'around the house in allover print', 'as im mockin deaf rock stars', 'at i got somethin to feed', 'beat deshay up with the stack', 'bedrock harder than a motherfuckin flintstone', 'bitchin this isnt a fuckin hotline', 'black kids never wanted to read', 'bloggin faggot hipster with a pitchfork', 'books the black kids never wanted', 'bowling i went home for some', 'bruno mars in his goddamn esophagus', 'bunnyhopped off my shoulder now my', 'but after bowling i went home', 'but i dont give a fuck', 'but where the fat ones at', 'called he said hes sick of', 'cinnamon ima scribble this sin and', 'clockin three past six and goin', 'cooking books the black kids never', 'crack rocks outta pussy nigga fishbones', 'crash that fuckin airplane that that', 'damn', 'danced around the house in allover', 'dead', 'deshay up with the stack of', 'diamonds and bitches dont need em', 'dicks thats nine cocks that cock', 'do i slipped myself some pink', 'dont give a fuck so hes', 'fame and all the hype g', 'fat ones at i got somethin', 'five seven of his bitches in', 'for a fuckin shrink sheesh i', 'fuck her wolf haley robbin them', 'fuck man fuck the fame and', 'fuck money diamonds and bitches dont', 'fuck so hes probably just like', 'fuck the fame and all the', 'fuckin airplane that that faggot nigga', 'fuckin broad will never understand me', 'fuckin shrink sheesh i already got', 'fuckin walkin paradox no im not', 'fuckin workin i think im wastin', 'gay i just wanna boogie to', 'give a fuck so hes probably', 'goddamn goblin', 'gold teeth and pregnant golden retrievers', 'gon put that fuckin hole in', 'gone that fuckin broad will never', 'green paper gold teeth and pregnant', 'guidance that i had is splattered', 'he said hes sick of the', 'heres the number to my therapist', 'hes fuckin awesome with listenin haha', 'hes not fuckin workin i think', 'him all your problems hes fuckin', 'him to quit bitchin this isnt', 'his bitches in my bedroom hey', 'home for some damn adventure time', 'how about i start a team', 'i beat deshay up with the', 'i dont give a fuck so', 'i got somethin to feed em', 'i had is splattered on cement', 'i just wanna boogie to some', 'i just wanna know if my', 'i slipped myself some pink xannies', 'i start a team of leaders', 'i think im wastin my damn', 'i told him to quit bitchin', 'i want fuck money diamonds and', 'i went home for some damn', 'if my father would ever like', 'ill crash that fuckin airplane that', 'im a fuckin walkin paradox no', 'im an overachiever so how about', 'im clockin three past six and', 'im not gay i just wanna', 'im stabbin any bloggin faggot hipster', 'im wolf ace gon put that', 'im wolf that was me who', 'im wolf tyler put this fuckin', 'ima scribble this sin and shit', 'is tellin me that shes been', 'it bunnyhopped off my shoulder now', 'its been a couple months', 'its some cooking books the black', 'jesus called he said hes sick', 'just wanna boogie to some marvin', 'just wanna know if my father', 'kids never wanted to read em', 'know if my father would ever', 'louder than words let me try', 'making crack rocks outta pussy nigga', 'man fuck the fame and all', 'me that shes been getting intimate', 'me who shoved a cock in', 'moms gone that fuckin broad will', 'money diamonds and bitches dont need', 'my father would ever like me', 'my moms gone that fuckin broad', 'my shoulder now my conscience dead', 'nigga jasper tryin to get grown', 'no v tech shit or columbine', 'not again another critic writing report', 'not fuckin workin i think im', 'not gay i just wanna boogie', 'now the only guidance that i', 'of his bitches in my bedroom', 'of the dicks thats nine cocks', 'off my shoulder now my conscience', 'oh not again another critic writing', 'ones at i got somethin to', 'only guidance that i had is', 'overachiever so how about i start', 'paper gold teeth and pregnant golden', 'pick up stevie wonder to be', 'problems hes fuckin awesome with listenin', 'put that fuckin hole in my', 'put this fuckin knife in my', 'quit bitchin this isnt a fuckin', 'rappin as im mockin deaf rock', 'revenge of the dicks thats nine', 'rocks outta pussy nigga fishbones haha', 'said hes sick of the disses', 'say success is the best revenge', 'seven of his bitches in my', 'shes been getting intimate with men', 'shoved a cock in your bitch', 'shrink sheesh i already got mine', 'slipped myself some pink xannies yeah', 'snap back green chchchia fuckin leaves', 'so hes probably just like me', 'so how about i start a', 'so i beat deshay up with', 'some cooking books the black kids', 'speak louder than words let me', 'stab bruno mars in his goddamn', 'stabbin any bloggin faggot hipster with', 'stevie wonder to be the wide', 'still aint perm her fuckin weave', 'still suicidal i am', 'stop until the cops come in', 'swallow the cinnamon ima scribble this', 'syd is tellin me that shes', 'syd shut the fuck up', 'synthetic wigs made of anwars dreadlocks', 'tell him all your problems hes', 'tellin me that shes been getting', 'than words let me try this', 'that faggot nigga bob is in', 'that fuckin airplane that that faggot', 'that fuckin broad will never understand', 'that fuckin hole in my head', 'that i had is splattered on', 'that shes been getting intimate with', 'that that faggot nigga bob is', 'that was me who shoved a', 'thats nine cocks that cock s', 'the black kids never wanted to', 'the cinnamon ima scribble this sin', 'the dicks thats nine cocks that', 'the fame and all the hype', 'the fat ones at i got', 'the fuck man fuck the fame', 'the house in allover print panties', 'the number to my therapist shit', 'the only guidance that i had', 'the revenge of the dicks thats', 'the stack of magazines im in', 'then it bunnyhopped off my shoulder', 'they say success is the best', 'think im wastin my damn time', 'this aint no v tech shit', 'this fuckin knife in my hand', 'this nigga jasper tryin to get', 'this the revenge of the dicks', 'three past six and goin postal', 'threesomes with a fuckin triceratops reptar', 'tina still aint perm her fuckin', 'to be the wide receiver cool', 'to quit bitchin this isnt a', 'told him to quit bitchin this', 'tyler put this fuckin knife in', 'uh wolf haley golf fuckin wang', 'uh wolf haley golf wang yeah', 'uh wolf haley uh golf wang', 'up stevie wonder to be the', 'up with the stack of magazines', 'wanna know if my father would', 'want fuck money diamonds and bitches', 'was me who shoved a cock', 'wearin synthetic wigs made of anwars', 'went home for some damn adventure', 'what the fuck man fuck the', 'what you think of hayley williams', 'whatd you do i slipped myself', 'where the fat ones at i', 'while syd is tellin me that', 'who shoved a cock in your', 'with the stack of magazines im', 'wolf ace gon put that fuckin', 'wolf haley uh golf wang', 'wolf that was me who shoved', 'wolf tyler put this fuckin knife', 'wonder to be the wide receiver', 'wont stop until the cops come', 'words let me try this shit', 'workin i think im wastin my', 'you do i slipped myself some', 'you tell him all your problems', 'your problems hes fuckin awesome with']\n",
            "Initializing model...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'goblin the in my my em would time wang wang wang wang wang'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approach 2: XLNET"
      ],
      "metadata": {
        "id": "iXFdBfgtLOjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = open(\"/content/drive/MyDrive/teardrops on my guitar.txt\", \"r\", encoding = \"utf8\")\n",
        "test_input = test_input.read()\n",
        "test_input = test_input.replace(\"\\n\\n\", \"\\n\")\n",
        "test_input = test_input.replace(\"\\n\\n\", \"\\n\")\n",
        "#print(test_input)"
      ],
      "metadata": {
        "id": "IAUbabg3r1o3"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Credits: https://huggingface.co/docs/transformers/model_doc/xlnet; https://towardsdatascience.com/build-a-bidirectional-text-generator-with-xlnet-49d9d37b48a9\n",
        "\n",
        "# Initializing a XLNet configuration\n",
        "configuration = XLNetConfig()\n",
        "\n",
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
        "model = XLNetLMHeadModel.from_pretrained('xlnet-large-cased')\n",
        "\n",
        "# Padding text to help Transformer-XL and XLNet with short prompts\n",
        "# in https://github.com/rusiaaman/XLNet-gen#methodology\n",
        "# and https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e\n",
        "PADDING_TEXT = test_input\n",
        "\n",
        "torch.manual_seed(0)\n",
        "# We show how to setup inputs to predict a next token using a bi-directional context.\n",
        "# We will predict masked tokens\n",
        "input_ids = torch.tensor(tokenizer.encode(PADDING_TEXT + \"I gave you three apples. <mask> have <mask> apples in hands\", add_special_tokens=False)).unsqueeze(0)  \n",
        "\n",
        "targets = [ -6, -4]\n",
        "\n",
        "perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
        "perm_mask[0, :, targets] = 1.0  # Previous tokens don't see last token\n",
        "\n",
        "target_mapping = torch.zeros((1, len(targets), input_ids.shape[1]), dtype=torch.float)  \n",
        "\n",
        "target_mapping[0, 0, targets[0]] = 1.0  # Our first  prediction \n",
        "target_mapping[0, 1, targets[1]] = 1.0  # Our second  prediction \n",
        "\n",
        "input_ids_tensor = input_ids.to(\"cuda\")\n",
        "target_mapping_tensor = target_mapping.to(\"cuda\")\n",
        "perm_mask_tensor = perm_mask.to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "if torch.cuda.is_available(): model.to('cuda') #if we have a GPU \n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs = model(input_ids_tensor, perm_mask=perm_mask_tensor, target_mapping=target_mapping_tensor)\n",
        "next_token_logits = outputs[0]  # Output has shape [target_mapping.size(0), target_mapping.size(1), config.vocab_size]\n",
        "\n",
        "for j in range(len(targets)):\n",
        "  predicted_k_indexes = torch.topk(outputs[0][0][j],k=5)\n",
        "  predicted_logits_list = predicted_k_indexes[0] \n",
        "  predicted_indexes_list = predicted_k_indexes[1] \n",
        "    \n",
        "  print (\"predicted word:\",tokenizer.decode(input_ids[0][targets[j]].item()), j)\n",
        "  for i,item  in enumerate(predicted_indexes_list):\n",
        "      the_index = predicted_indexes_list[i].item()\n",
        "      print(\"word and logits\",tokenizer.decode(the_index),predicted_logits_list[i].item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ64vp19MdMU",
        "outputId": "52087ca4-9bcc-4e98-ea71-bf3058a20455"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicted word: <mask> 0\n",
            "word and logits I -17.119409561157227\n",
            "word and logits You -18.564960479736328\n",
            "word and logits I -20.358469009399414\n",
            "word and logits We -20.713825225830078\n",
            "word and logits You -21.69858741760254\n",
            "predicted word: <mask> 1\n",
            "word and logits three -19.076858520507812\n",
            "word and logits the -21.324016571044922\n",
            "word and logits two -22.618837356567383\n",
            "word and logits your -22.90503692626953\n",
            "word and logits my -23.162578582763672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to select topK tokens from the probability list and \n",
        "# then based on the selected K word distribution get sample of random token IDs\n",
        "def choose_from_top(probs, k=5, sample_size=1):\n",
        "    ind = np.argpartition(probs, -k)[-k:]\n",
        "    top_prob = probs[ind]\n",
        "    # print(tokenizer.decode(ind))\n",
        "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
        "    choice = np.random.choice(k, sample_size, p = top_prob, replace=False)\n",
        "    token_ids = ind[choice]\n",
        "    return token_ids\n",
        "\n",
        "sent = \"So I drive home alone\"\n",
        "topk = 10\n",
        "n = 20\n",
        "# Lower temperatures make the model more confident in its top choices, while temperatures greater than 1 decrease confidence.\n",
        "temperature = 5\n",
        "\n",
        "model.eval()\n",
        "if torch.cuda.is_available(): model.to('cuda') #if we have a GPU \n",
        "\n",
        "sent_tokens = tokenizer.encode(sent, add_special_tokens=False)\n",
        "mask_tokens = tokenizer.encode('<mask>', add_special_tokens=False)\n",
        "padding_tokens = tokenizer.encode(PADDING_TEXT, add_special_tokens=False)\n",
        "   \n",
        "for i in range(n):\n",
        "  input = mask_tokens + sent_tokens + mask_tokens     \n",
        "  target_id1 = -len(input)\n",
        "  target_id2 = -1\n",
        "\n",
        "  input_ids = torch.tensor(padding_tokens + input).unsqueeze(0)   # We will predict masked tokens\n",
        "\n",
        "  perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
        "  perm_mask[0, :, [target_id1, target_id2]] = 1.0  # Previous tokens don't see last token\n",
        "\n",
        "  target_mapping = torch.zeros((1, 2, input_ids.shape[1]), dtype=torch.float)  \n",
        "  target_mapping[0, 0, target_id1] = 1.0  # Our first  prediction \n",
        "  target_mapping[0, 1, target_id2] = 1.0  # Our second  prediction \n",
        "\n",
        "  input_ids_tensor = input_ids.to(\"cuda\")\n",
        "  target_mapping_tensor = target_mapping.to(\"cuda\")\n",
        "  perm_mask_tensor = perm_mask.to(\"cuda\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_ids_tensor, perm_mask=perm_mask_tensor, target_mapping=target_mapping_tensor)\n",
        "\n",
        "  predicted_tokens = []\n",
        "  \n",
        "  for j in range(2):\n",
        "    probs = torch.nn.functional.softmax(outputs[0][0][j]/temperature, dim = 0).to('cpu').numpy()\n",
        "    predicted_tokens.append(choose_from_top(probs, k=topk, sample_size=1))\n",
        "\n",
        "  if i % 2 == 0:    \n",
        "    tok = predicted_tokens[0][0]\n",
        "    sent_tokens = [tok] + sent_tokens \n",
        "    print('left: ', tokenizer.decode(sent_tokens))\n",
        "  else:     \n",
        "    tok = predicted_tokens[1][0]\n",
        "    sent_tokens = sent_tokens + [tok]\n",
        "    print(\"right: \", tokenizer.decode(sent_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCne0ujLmYNW",
        "outputId": "e613eeb2-7d7b-4621-bdd1-cf703ddeeb45"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "left:  it So I drive home alone\n",
            "right:  it So I drive home alone\n",
            "left:  through it So I drive home alone\n",
            "right:  through it So I drive home alone -\n",
            "left:  my through it So I drive home alone -\n",
            "right:  my through it So I drive home alone - a\n",
            "left:  I my through it So I drive home alone - a\n",
            "right:  I my through it So I drive home alone - a song\n",
            "left:  That I my through it So I drive home alone - a song\n",
            "right:  That I my through it So I drive home alone - a song of\n",
            "left:  See That I my through it So I drive home alone - a song of\n",
            "right:  See That I my through it So I drive home alone - a song of hope\n",
            "left:  me See That I my through it So I drive home alone - a song of hope\n",
            "right:  me See That I my through it So I drive home alone - a song of hope A\n",
            "left:  Behind me See That I my through it So I drive home alone - a song of hope A\n",
            "right:  Behind me See That I my through it So I drive home alone - a song of hope A cry\n",
            "left:  Looking Behind me See That I my through it So I drive home alone - a song of hope A cry\n",
            "right:  Looking Behind me See That I my through it So I drive home alone - a song of hope A cry of\n",
            "left:  But Looking Behind me See That I my through it So I drive home alone - a song of hope A cry of\n",
            "right:  But Looking Behind me See That I my through it So I drive home alone - a song of hope A cry of anguish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a combination of beam and top-k generation to generate sequences of n tokens from both sides \n",
        "\n",
        "import random\n",
        "\n",
        "padding_tokens = tokenizer.encode(PADDING_TEXT, add_special_tokens=False)\n",
        "mask_tokens = tokenizer.encode('<mask>', add_special_tokens=False)\n",
        "\n",
        "model.eval()\n",
        "if torch.cuda.is_available(): model.to('cuda') #if we have a GPU \n",
        "\n",
        "def candidates_gen(sent_tokens, candidate=([], 1, []), d='left', n_candidates=5, topk=20, temperature=5):\n",
        "  branch_candidates = []  \n",
        "  cand_tokens = candidate[0]\n",
        "  \n",
        "  if d == 'right':    \n",
        "    input = sent_tokens + cand_tokens + mask_tokens     \n",
        "    \n",
        "    target_id = -1\n",
        "    input_ids = torch.tensor(padding_tokens + input).unsqueeze(0)  \n",
        "\n",
        "    perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
        "    perm_mask[0, :, target_id] = 1.0  # Previous tokens don't see last token\n",
        "  else:        \n",
        "    input = mask_tokens + cand_tokens + sent_tokens    \n",
        "    \n",
        "    target_id = -len(input)  \n",
        "    input_ids = torch.tensor(padding_tokens + input).unsqueeze(0)  \n",
        "\n",
        "    perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
        "    perm_mask[0, :, [target_id - i for i in range(100)]] = 1.0  # Mask additional previos tokens to improve left-side generation\n",
        "\n",
        "  # We will predict masked tokens \n",
        "  target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float)  \n",
        "  target_mapping[0, 0, target_id] = 1.0  # Our right  prediction \n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    input_ids_tensor = input_ids.to(\"cuda\")\n",
        "    target_mapping_tensor = target_mapping.to(\"cuda\")\n",
        "    perm_mask_tensor = perm_mask.to(\"cuda\")\n",
        "  else:\n",
        "    input_ids_tensor = input_ids\n",
        "    target_mapping_tensor = target_mapping\n",
        "    perm_mask_tensor = perm_mask\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_ids_tensor, perm_mask=perm_mask_tensor, target_mapping=target_mapping_tensor)\n",
        "\n",
        "  probs = torch.nn.functional.softmax(outputs[0][0][0]/temperature, dim = 0)\n",
        "  selected_indexes = choose_from_top(probs.to('cpu').numpy(), k=topk, sample_size=n_candidates)\n",
        "  selected_probs = probs[selected_indexes]\n",
        "\n",
        "  for i,item  in enumerate(selected_indexes):\n",
        "      the_index = item.item()\n",
        "      if d == \"right\":\n",
        "        new_sent = cand_tokens + [the_index]\n",
        "      elif d == \"left\":\n",
        "        new_sent = [the_index] + cand_tokens\n",
        "      \n",
        "      prob = selected_probs[i].item()\n",
        "      # add word combinations to branch_candidates in format [sentence, cumulative probability, all probs]\n",
        "      branch_candidates.append((new_sent, candidate[1] * prob, candidate[2] + [prob]))\n",
        "  \n",
        "  return branch_candidates\n",
        "\n",
        "def beam_gen(sent_tokens, candidates, depth=5, d='right', sample_size=2, topk=10, temperature=5):\n",
        "  beams = candidates[:]\n",
        "  new_candidates = candidates[:]\n",
        "  while depth > 0:\n",
        "    new_candidates = []\n",
        "    for candidate in candidates:\n",
        "      for new_candidate in candidates_gen(sent_tokens, candidate, d, sample_size, topk, temperature):\n",
        "        beams.append(new_candidate)\n",
        "        new_candidates.append(new_candidate)   \n",
        "    print(\"Number of beams:\", len(new_candidates))    \n",
        "    candidates = new_candidates[:]\n",
        "    depth -= 1\n",
        "  sorted_beams = sorted(new_candidates, key=lambda tup: np.sum(np.log10(tup[2])), reverse=True)\n",
        "  return beams, sorted_beams\n",
        "\n",
        "def bi_generator(sent, direction, first_sample_size, sample_size, n_tokens, topk, iterations, temperature):\n",
        "  sent_tokens = tokenizer.encode(sent, add_special_tokens=False) \n",
        "\n",
        "  for i in range(iterations):\n",
        "    if (i % 2 == 0 and direction == 'both') or direction == 'left':\n",
        "      print('>> left side generation')\n",
        "      candidates = candidates_gen(sent_tokens=sent_tokens, d='left', n_candidates=first_sample_size,  topk=topk, temperature=temperature)\n",
        "      beams, sorted_beams = beam_gen(sent_tokens, candidates, n_tokens-1, 'left', sample_size, topk, temperature=temperature)\n",
        "      topn = len(sorted_beams)//5 if len(sorted_beams) > 4 else len(sorted_beams)\n",
        "      selected_candidate = random.choice(sorted_beams[:topn])\n",
        "      sent_tokens = selected_candidate[0] + sent_tokens\n",
        "      print(tokenizer.decode(sent_tokens))\n",
        "    if (i % 2 != 0 and direction == 'both') or direction == 'right':\n",
        "      print('>> right side generation')\n",
        "      candidates = candidates_gen(sent_tokens=sent_tokens, d='right', n_candidates=first_sample_size, topk=topk, temperature=temperature)\n",
        "      beams, sorted_beams = beam_gen(sent_tokens, candidates, n_tokens-1, 'right', sample_size, topk, temperature=temperature)\n",
        "      topn = len(sorted_beams)//5 if len(sorted_beams) > 4 else len(sorted_beams)\n",
        "      selected_candidate = random.choice(sorted_beams[:topn])\n",
        "      sent_tokens = sent_tokens + selected_candidate[0]\n",
        "      print(tokenizer.decode(sent_tokens))\n",
        "    \n",
        "  return tokenizer.decode(sent_tokens)\n",
        "\n",
        "sent = \"Jesus called, he said he's sick of the disses\"  \n",
        "first_sample_size = 4\n",
        "sample_size = 2\n",
        "n_tokens = 4\n",
        "topk = 20\n",
        "iterations = 6\n",
        "temperature = 4\n",
        "direction = \"both\"\n",
        "\n",
        "bi_generator(sent, direction, first_sample_size, sample_size, n_tokens, topk, iterations, temperature);\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "budZ8DEgt5IN",
        "outputId": "4efe5b5a-923b-4c94-8b41-9465f8f4a512"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> left side generation\n",
            "Number of beams: 8\n",
            "Number of beams: 16\n",
            "Number of beams: 32\n",
            "'re dead But Jesus called, he said he's sick of the disses\n",
            ">> right side generation\n",
            "Number of beams: 8\n",
            "Number of beams: 16\n",
            "Number of beams: 32\n",
            "'re dead But Jesus called, he said he's sick of the disses What do he have\n",
            ">> left side generation\n",
            "Number of beams: 8\n",
            "Number of beams: 16\n",
            "Number of beams: 32\n",
            "we all know we're dead But Jesus called, he said he's sick of the disses What do he have\n",
            ">> right side generation\n",
            "Number of beams: 8\n",
            "Number of beams: 16\n",
            "Number of beams: 32\n",
            "we all know we're dead But Jesus called, he said he's sick of the disses What do he have no right to ask\n",
            ">> left side generation\n",
            "Number of beams: 8\n",
            "Number of beams: 16\n",
            "Number of beams: 32\n",
            "me tacause we all know we're dead But Jesus called, he said he's sick of the disses What do he have no right to ask\n",
            ">> right side generation\n",
            "Number of beams: 8\n",
            "Number of beams: 16\n",
            "Number of beams: 32\n",
            "me tacause we all know we're dead But Jesus called, he said he's sick of the disses What do he have no right to ask What is fuckin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approach 3: GPT-2"
      ],
      "metadata": {
        "id": "G9f-wBnIcRfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "#Accumulated batch size (since GPT2 is so big)\n",
        "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
        "    if packed_tensor is None:\n",
        "        return new_tensor, True, None\n",
        "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
        "        return packed_tensor, False, new_tensor\n",
        "    else:\n",
        "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
        "        return packed_tensor, True, None\n",
        "\n",
        "### Prepare data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/adele.txt\", sep=\"\\n\", header=None)\n",
        "\n",
        "class SongLyrics(Dataset):  \n",
        "    def __init__(self, control_code, truncate=False, gpt2_type=\"gpt2\", max_length=1024):\n",
        "\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
        "        self.lyrics = []\n",
        "\n",
        "        for row in df['Lyric']:\n",
        "          self.lyrics.append(torch.tensor(\n",
        "                self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")\n",
        "            ))               \n",
        "        if truncate:\n",
        "            self.lyrics = self.lyrics[:20000]\n",
        "        self.lyrics_count = len(self.lyrics)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.lyrics_count\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.lyrics[item]\n",
        "    \n",
        "dataset = df"
      ],
      "metadata": {
        "id": "HdbKT-FpcP9c"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    dataset, model, tokenizer,\n",
        "    batch_size=16, epochs=5, lr=2e-5,\n",
        "    max_seq_len=400, warmup_steps=200,\n",
        "    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n",
        "    test_mode=False,save_model_on_epoch=False,\n",
        "):\n",
        "    acc_steps = 100\n",
        "    device=torch.device(\"cuda\")\n",
        "    model = model.cuda()\n",
        "    model.train()\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
        "    )\n",
        "\n",
        "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "    loss=0\n",
        "    accumulating_batch_count = 0\n",
        "    input_tensor = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        print(f\"Training epoch {epoch}\")\n",
        "        print(loss)\n",
        "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
        "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
        "\n",
        "            if carry_on and idx != len(train_dataloader) - 1:\n",
        "                continue\n",
        "\n",
        "            input_tensor = input_tensor.to(device)\n",
        "            outputs = model(input_tensor, labels=input_tensor)\n",
        "            loss = outputs[0]\n",
        "            loss.backward()\n",
        "\n",
        "            if (accumulating_batch_count % batch_size) == 0:\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                model.zero_grad()\n",
        "\n",
        "            accumulating_batch_count += 1\n",
        "            input_tensor = None\n",
        "        if save_model_on_epoch:\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
        "            )\n",
        "    return model\n",
        "\n",
        "model = train(dataset, model, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "UOHIM2dilZwz",
        "outputId": "2cb3fb1b-b138-4f6d-d0c2-42b3d7b9e1db"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 0\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 180",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-ddf6f7703a85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-ddf6f7703a85>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, model, tokenizer, batch_size, epochs, lr, max_seq_len, warmup_steps, gpt2_type, output_dir, output_prefix, test_mode, save_model_on_epoch)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training epoch {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcarry_on\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremainder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 180"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOewcK2Z8ypdnNv3P05jioq"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}