{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p1cRiAOIDMG"
      },
      "source": [
        "Lyric Generation: 3 Ways (XLNET, GPT-2, LSTM with Pytorch)\n",
        "\n",
        "Collaborators: Aleks Sekulovski + Grayson Taylor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyFEcFvzYOxq",
        "outputId": "bf8f35a2-e186-412e-a747-c6e2b5ea75e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_JZIgd3HgCER",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8ac21857-0faf-4cde-850c-2d8eed15f63d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.8/dist-packages (1.1.4)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.8/dist-packages (from flask) (1.0.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.8/dist-packages (from flask) (2.11.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.8/dist-packages (from flask) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.8/dist-packages (from flask) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2<3.0,>=2.10.1->flask) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spotipy in /usr/local/lib/python3.8/dist-packages (2.22.0)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.8/dist-packages (from spotipy) (2.28.1)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.8/dist-packages (from spotipy) (1.26.13)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spotipy) (1.15.0)\n",
            "Requirement already satisfied: redis>=3.5.3 in /usr/local/lib/python3.8/dist-packages (from spotipy) (4.4.0)\n",
            "Requirement already satisfied: async-timeout>=4.0.2 in /usr/local/lib/python3.8/dist-packages (from redis>=3.5.3->spotipy) (4.0.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.25.0->spotipy) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.25.0->spotipy) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.25.0->spotipy) (2022.9.24)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.8/dist-packages (0.1.91)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gpt-2-simple in /usr/local/lib/python3.8/dist-packages (0.8.1)\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.8/dist-packages (from gpt-2-simple) (2.9.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from gpt-2-simple) (1.21.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from gpt-2-simple) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gpt-2-simple) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from gpt-2-simple) (2.28.1)\n",
            "Requirement already satisfied: toposort in /usr/local/lib/python3.8/dist-packages (from gpt-2-simple) (1.7)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.12)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.1.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (14.0.6)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.9.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.1.1)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.9.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (21.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.51.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (57.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.19.6)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.14.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.28.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.38.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (2.15.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (0.4.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->gpt-2-simple) (1.26.13)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->gpt-2-simple) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->gpt-2-simple) (2022.9.24)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->gpt-2-simple) (2.1.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow>=2.5.1->gpt-2-simple) (3.0.9)\n",
            "Tue Dec 13 21:52:33 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    52W / 400W |    632MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.7.1-py3-none-any.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.28.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.11.1)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 99.2 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 91.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.26.13)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, responses, multiprocess, datasets\n",
            "Successfully installed datasets-2.7.1 multiprocess-0.70.14 responses-0.18.0 xxhash-3.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting huggingface-hub==0.7\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub==0.7) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub==0.7) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub==0.7) (2.28.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from huggingface-hub==0.7) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub==0.7) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub==0.7) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub==0.7) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub==0.7) (2022.9.24)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub==0.7) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub==0.7) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub==0.7) (1.26.13)\n",
            "Installing collected packages: huggingface-hub\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.11.1\n",
            "    Uninstalling huggingface-hub-0.11.1:\n",
            "      Successfully uninstalled huggingface-hub-0.11.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.25.1 requires huggingface-hub<1.0,>=0.10.0, but you have huggingface-hub 0.7.0 which is incompatible.\u001b[0m\n",
            "Successfully installed huggingface-hub-0.7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.6-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 4.6 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 83.3 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.11.1-py2.py3-none-any.whl (168 kB)\n",
            "\u001b[K     |████████████████████████████████| 168 kB 83.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.28.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.3 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.13)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=32f127347bf9ae04dbc70bb788ea557f99cd5804edf27ec0c12434dd9c341428\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.29 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.11.1 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 wandb-0.13.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lyricsgenius in /usr/local/lib/python3.8/dist-packages (3.0.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from lyricsgenius) (4.6.3)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.8/dist-packages (from lyricsgenius) (2.28.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20.0->lyricsgenius) (1.26.13)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20.0->lyricsgenius) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20.0->lyricsgenius) (2022.9.24)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20.0->lyricsgenius) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (3.8.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp) (22.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp) (6.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp) (2.1.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from langdetect) (1.15.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=9940dc4e3de9ad8e2189e6fcc329341c64061d303f2e989350b68be47a877081\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/c7/b0/79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.15.0-py3-none-any.whl (191 kB)\n",
            "\u001b[K     |████████████████████████████████| 191 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.21.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate) (5.4.8)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.13.0+cu116)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.4.0->accelerate) (4.4.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.15.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.8/dist-packages (0.3.25)\n",
            "Collecting jax\n",
            "  Downloading jax-0.4.1.tar.gz (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jaxlib in /usr/local/lib/python3.8/dist-packages (0.3.25+cuda11.cudnn805)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.4.1-cp38-cp38-manylinux2014_x86_64.whl (71.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 71.3 MB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from jax) (1.21.6)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.8/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax) (1.7.3)\n",
            "Building wheels for collected packages: jax\n",
            "  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.4.1-py3-none-any.whl size=1332478 sha256=2a63873ca89e4f83f1b8ca4ea2775b763e5c55d7b8f78a816459e6b3ec305e5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/c2/cc/ac766a5c8ed28aec746fc292690d3e2e3790b554d2a6abacb7\n",
            "Successfully built jax\n",
            "Installing collected packages: jaxlib, jax\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.3.25+cuda11.cudnn805\n",
            "    Uninstalling jaxlib-0.3.25+cuda11.cudnn805:\n",
            "      Successfully uninstalled jaxlib-0.3.25+cuda11.cudnn805\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.3.25\n",
            "    Uninstalling jax-0.3.25:\n",
            "      Successfully uninstalled jax-0.3.25\n",
            "Successfully installed jax-0.4.1 jaxlib-0.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "jax",
                  "jaxlib"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/google/flax.git\n",
            "  Cloning https://github.com/google/flax.git to /tmp/pip-req-build-rz8siywk\n",
            "  Running command git clone -q https://github.com/google/flax.git /tmp/pip-req-build-rz8siywk\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.8/dist-packages (from flax==0.6.3) (1.21.6)\n",
            "Requirement already satisfied: jax>=0.3.16 in /usr/local/lib/python3.8/dist-packages (from flax==0.6.3) (0.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from flax==0.6.3) (3.2.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.8/dist-packages (from flax==0.6.3) (1.0.4)\n",
            "Collecting optax\n",
            "  Downloading optax-0.1.4-py3-none-any.whl (154 kB)\n",
            "\u001b[K     |████████████████████████████████| 154 kB 4.5 MB/s \n",
            "\u001b[?25hCollecting orbax\n",
            "  Downloading orbax-0.0.23-py3-none-any.whl (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting tensorstore\n",
            "  Downloading tensorstore-0.1.28-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.3 MB 39.0 MB/s \n",
            "\u001b[?25hCollecting rich>=11.1\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 68.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from flax==0.6.3) (4.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.8/dist-packages (from flax==0.6.3) (6.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.16->flax==0.6.3) (1.7.3)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.16->flax==0.6.3) (3.3.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich>=11.1->flax==0.6.3) (2.6.1)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->flax==0.6.3) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->flax==0.6.3) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->flax==0.6.3) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->flax==0.6.3) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->flax==0.6.3) (1.15.0)\n",
            "Collecting chex>=0.1.5\n",
            "  Downloading chex-0.1.5-py3-none-any.whl (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.8/dist-packages (from optax->flax==0.6.3) (0.4.1)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from optax->flax==0.6.3) (1.3.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax->flax==0.6.3) (0.1.7)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax->flax==0.6.3) (0.12.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.8/dist-packages (from orbax->flax==0.6.3) (5.10.0)\n",
            "Requirement already satisfied: etils in /usr/local/lib/python3.8/dist-packages (from orbax->flax==0.6.3) (0.9.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.8/dist-packages (from orbax->flax==0.6.3) (3.6.4)\n",
            "Collecting cached_property\n",
            "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib_resources->orbax->flax==0.6.3) (3.11.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax==0.6.3) (1.4.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax==0.6.3) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax==0.6.3) (1.11.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax==0.6.3) (22.1.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax==0.6.3) (9.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax==0.6.3) (57.4.0)\n",
            "Building wheels for collected packages: flax\n",
            "  Building wheel for flax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flax: filename=flax-0.6.3-py3-none-any.whl size=198662 sha256=0a49db1405c255d9aee84ac2dd5da233be66c6c33f6de7239bb54111c01199f5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-btnhtlly/wheels/5d/cd/13/31d6d37c2228329484735564bfa16a48768d2232924588f91b\n",
            "Successfully built flax\n",
            "Installing collected packages: tensorstore, commonmark, chex, cached-property, rich, orbax, optax, flax\n",
            "Successfully installed cached-property-1.5.2 chex-0.1.5 commonmark-0.9.1 flax-0.6.3 optax-0.1.4 orbax-0.0.23 rich-12.6.0 tensorstore-0.1.28\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hf-lfs\n",
            "  Downloading hf_lfs-0.0.3-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 4.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: hf-lfs\n",
            "Successfully installed hf-lfs-0.0.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nest_asyncio\n",
            "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
            "Installing collected packages: nest-asyncio\n",
            "Successfully installed nest-asyncio-1.5.6\n"
          ]
        }
      ],
      "source": [
        "# installs\n",
        "!pip install flask\n",
        "!pip install spotipy\n",
        "!pip install transformers\n",
        "!pip install sentencepiece==0.1.91\n",
        "!pip install gpt-2-simple\n",
        "!nvidia-smi\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install huggingface-hub==0.7\n",
        "!pip install torch\n",
        "!pip install wandb\n",
        "!pip install lyricsgenius\n",
        "!pip install aiohttp\n",
        "!pip install langdetect\n",
        "!pip install accelerate\n",
        "!pip install --upgrade jax jaxlib \n",
        "!pip install --upgrade git+https://github.com/google/flax.git\n",
        "!pip install tqdm --upgrade\n",
        "!pip install hf-lfs\n",
        "!pip install nest_asyncio\n",
        "\n",
        "# imports\n",
        "from transformers import XLNetConfig, XLNetModel, XLNetLMHeadModel, XLNetTokenizer\n",
        "import torch\n",
        "import lyricsgenius\n",
        "import spotipy\n",
        "from spotipy.oauth2 import SpotifyClientCredentials\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import normalize\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from IPython.display import display, HTML, Javascript, clear_output\n",
        "import lyricsgenius\n",
        "from tqdm.notebook import tqdm as bar\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from datasets import Dataset, DatasetDict\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "import langdetect\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "import wandb\n",
        "import pathlib\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import asyncio\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import sys\n",
        "sys.setrecursionlimit(99999)\n",
        "import aiohttp\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import urllib.request\n",
        "import urllib.parse\n",
        "import json\n",
        "import flask\n",
        "import gpt_2_simple as gpt2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as Functional\n",
        "import numpy as np\n",
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approach 1: LSTM Model (Pytorch)"
      ],
      "metadata": {
        "id": "DvrayUQ7K2xR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YHaNSl9QKK8g"
      },
      "outputs": [],
      "source": [
        "### Credits: https://www.analyticsvidhya.com/blog/2020/08/build-a-natural-language-generation-nlg-system-using-pytorch/\n",
        "\n",
        "# clean alphabet for word generation\n",
        "def clean_lyric(lyric):\n",
        "    return re.sub(\"[^a-z' ]\", \"\", lyric).replace(\"'\", \"\")\n",
        "\n",
        "# create and return list of all possible sequences\n",
        "def create_sequences(lyric, seq_len):\n",
        "    sequences = []\n",
        "    if len(lyric.split()) <= seq_len:\n",
        "        return [lyric]\n",
        "    for itr in range(seq_len, len(lyric.split())):\n",
        "        curr_seq = lyric.split()[itr - seq_len:itr + 1]\n",
        "        sequences.append(\" \".join(curr_seq))\n",
        "    return sequences\n",
        "\n",
        "# gets numbers for words in input sequence\n",
        "def get_seq_idx(sequence):\n",
        "    return [word_to_idx[word] for word in sequence.split()]\n",
        "\n",
        "# gets the next batch\n",
        "def get_next_batch(x, y, batch_size):\n",
        "    for itr in range(batch_size, x.shape[0], batch_size):\n",
        "        batch_x = x[itr - batch_size:itr, :]\n",
        "        batch_y = y[itr - batch_size:itr, :]\n",
        "        yield batch_x, batch_y\n",
        "\n",
        "# make inputs, detach hidden state, get model output, get token Ps and reshape\n",
        "# get top 3 values, select 1 and return it (along with the hidden state)\n",
        "def predict(model, token, hidden_layer):\n",
        "    x = np.array([[word_to_idx[token]]])\n",
        "    inputs = torch.from_numpy(x).type(torch.LongTensor)\n",
        "    hidden = tuple([layer.data for layer in hidden_layer])\n",
        "    out, hidden = model(inputs, hidden)\n",
        "    prob = Functional.softmax(out, dim=1).data.numpy()\n",
        "    prob = prob.reshape(prob.shape[1],)\n",
        "    top_tokens = prob.argsort()[-3:][::-1]\n",
        "    selected_index = top_tokens[0]\n",
        "    return idx_to_word[selected_index], hidden\n",
        "\n",
        "# creates the string\n",
        "def generate(model, num_words, start_text):\n",
        "    model.eval()\n",
        "    hidden = model.init_hidden(1)\n",
        "    tokens = start_text.split()\n",
        "    for token in start_text.split():\n",
        "        curr_token, hidden = predict(model, token, hidden)\n",
        "    tokens.append(curr_token)\n",
        "    for token_num in range(num_words - 1):\n",
        "        token, hidden = predict(model, tokens[-1], hidden)\n",
        "        tokens.append(token)\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# calls generate to create output string\n",
        "def get_lyric(start_text, num_words):\n",
        "    return generate(model, num_words, start_text.lower())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the meaty part (LSTM)\n",
        "class LSTM(nn.Module):\n",
        "    # initialize variables\n",
        "    def __init__(self, num_hidden, num_layers, embed_size, drop_prob, lr):\n",
        "        #print(\"Initializing model...\")\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.num_layers = num_layers\n",
        "        self.num_hidden = num_hidden\n",
        "        self.lr = lr\n",
        "        # define embedded layer, LSTM, dropout layer, andd fully connected layer\n",
        "        self.embedded = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, num_hidden, num_layers, dropout = drop_prob, batch_first = True)\n",
        "        self.dropout = nn.Dropout(drop_prob) \n",
        "        self.fc = nn.Linear(num_hidden, vocab_size)      \n",
        "    \n",
        "    # forward propagate\n",
        "    def forward(self, x, hidden_layer):      \n",
        "        # pass input through embedding layer\n",
        "        embedded = self.embedded(x)        \n",
        "        # get outputs and hidden layer from LSTM layer\n",
        "        lstm_output, hidden_layer = self.lstm(embedded, hidden_layer)        \n",
        "        # pass through a dropout layer and reshape\n",
        "        dropout_out = self.dropout(lstm_output).reshape(-1, self.num_hidden) \n",
        "        # put \"out\" through the fully-connected layer\n",
        "        out = self.fc(dropout_out)\n",
        "        # return final output and hidden layer\n",
        "        return out, hidden_layer\n",
        "    \n",
        "    # init hidden layer\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Create a weight torch using the parameters of the model\n",
        "        weight = next(self.parameters()).data\n",
        "        # initialize the hidden layer using the weight torch\n",
        "        hidden_layer = (weight.new(self.num_layers, batch_size, self.num_hidden).zero_(), weight.new(self.num_layers, batch_size, self.num_hidden).zero_())\n",
        "        # return the hidden layer\n",
        "        return hidden_layer"
      ],
      "metadata": {
        "id": "V8Rv76RqlO2I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/drive/MyDrive/adele.txt\", \"r\", encoding = \"utf8\")\n",
        "text = file.read()\n",
        "text = text.replace(\"\\n\\n\", \"\\n\")\n",
        "\n",
        "lyrics = text.lower().split(\"\\n\")\n",
        "lyrics = np.unique(lyrics)[1:].tolist()\n",
        "\n",
        "cleaned_lyrics = [clean_lyric(lyric) for lyric in lyrics]\n",
        "\n",
        "seq_size = 5\n",
        "\n",
        "# get every sequence\n",
        "raw_sequences = [create_sequences(lyric, seq_size) for lyric in cleaned_lyrics]\n",
        "\n",
        "# unique sentences only\n",
        "sequences = np.unique(np.array(sum(raw_sequences, []))).tolist()\n",
        "print(sequences)\n",
        "\n",
        "uniq_words = np.unique(np.array(\" \".join(sequences).split(\" \")))\n",
        "uniq_words_idx = np.arange(uniq_words.size)\n",
        "\n",
        "word_to_idx = dict(zip(uniq_words.tolist(), uniq_words_idx.tolist()))\n",
        "idx_to_word = dict(zip(uniq_words_idx.tolist(), uniq_words.tolist()))\n",
        "\n",
        "vocab_size = len(word_to_idx)\n",
        "\n",
        "# intialize empty lists\n",
        "x_word = []\n",
        "y_word = []\n",
        "\n",
        "# iterate through every sequence\n",
        "for seq in sequences:\n",
        "    if (len(seq.split()) != seq_size + 1):\n",
        "        continue\n",
        "    # add words to sequences\n",
        "    x_word.append(\" \".join(seq.split()[:-1]))\n",
        "    y_word.append(\" \".join(seq.split()[1:]))\n",
        "\n",
        "x_idx = np.array([get_seq_idx(word) for word in x_word])\n",
        "y_idx = np.array([get_seq_idx(word) for word in y_word])\n",
        "\n",
        "# initialize parameters for LSTM model\n",
        "num_hidden = 256\n",
        "num_layers = 4\n",
        "embed_size = 200\n",
        "drop_prob = 0.3\n",
        "lr = 0.001\n",
        "num_epochs = 30\n",
        "batch_size = 30\n",
        "\n",
        "# create LSTM model, choosing optimizer and loss function, and training model\n",
        "model = LSTM(num_hidden, num_layers, embed_size, drop_prob, lr)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # initialize hidden state\n",
        "    hidden_layer = model.init_hidden(batch_size)\n",
        "        \n",
        "    for x, y in get_next_batch(x_idx, y_idx, batch_size):            \n",
        "        # numpy -> Pytorch\n",
        "        inputs = torch.from_numpy(x).type(torch.LongTensor)\n",
        "        act = torch.from_numpy(y).type(torch.LongTensor)\n",
        "        # reformat hidden layer\n",
        "        hidden_layer = tuple([layer.data for layer in hidden_layer])\n",
        "        # get zero-accumulated gradients from the model\n",
        "        model.zero_grad()            \n",
        "        # get output\n",
        "        output, hidden = model(inputs, hidden_layer)           \n",
        "        # calculate loss from this prediction\n",
        "        loss = loss_func(output, act.view(-1))\n",
        "        # backpropagation\n",
        "        loss.backward()\n",
        "        # prevents exploding gradient problem\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "        optimizer.step()    \n",
        "\n",
        "get_lyric(\"love\", 8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "RUOzzIWYkfmj",
        "outputId": "da4d370c-cbd3-455f-e0eb-b53e9563ed78"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a fuck so hes probably just', 'a fuckin shrink sheesh i already', 'a fuckin walkin paradox no im', 'a motherfuckin goblin', 'about five seven of his bitches', 'about i start a team of', 'ace gon put that fuckin hole', 'actions speak louder than words let', 'after bowling i went home for', 'aint no v tech shit or', 'airplane that that faggot nigga bob', 'all i want fuck money diamonds', 'all your problems hes fuckin awesome', 'an overachiever so how about i', 'and danced around the house in', 'and hes not fuckin workin i', 'and im wolf that was me', 'and pick up stevie wonder to', 'and stab bruno mars in his', 'and tina still aint perm her', 'and wont stop until the cops', 'any bloggin faggot hipster with a', 'around the house in allover print', 'as im mockin deaf rock stars', 'at i got somethin to feed', 'beat deshay up with the stack', 'bedrock harder than a motherfuckin flintstone', 'bitchin this isnt a fuckin hotline', 'black kids never wanted to read', 'bloggin faggot hipster with a pitchfork', 'books the black kids never wanted', 'bowling i went home for some', 'bruno mars in his goddamn esophagus', 'bunnyhopped off my shoulder now my', 'but after bowling i went home', 'but i dont give a fuck', 'but where the fat ones at', 'called he said hes sick of', 'cinnamon ima scribble this sin and', 'clockin three past six and goin', 'cooking books the black kids never', 'crack rocks outta pussy nigga fishbones', 'crash that fuckin airplane that that', 'damn', 'danced around the house in allover', 'dead', 'deshay up with the stack of', 'diamonds and bitches dont need em', 'dicks thats nine cocks that cock', 'do i slipped myself some pink', 'dont give a fuck so hes', 'fame and all the hype g', 'fat ones at i got somethin', 'five seven of his bitches in', 'for a fuckin shrink sheesh i', 'fuck her wolf haley robbin them', 'fuck man fuck the fame and', 'fuck money diamonds and bitches dont', 'fuck so hes probably just like', 'fuck the fame and all the', 'fuckin airplane that that faggot nigga', 'fuckin broad will never understand me', 'fuckin shrink sheesh i already got', 'fuckin walkin paradox no im not', 'fuckin workin i think im wastin', 'gay i just wanna boogie to', 'give a fuck so hes probably', 'goddamn goblin', 'gold teeth and pregnant golden retrievers', 'gon put that fuckin hole in', 'gone that fuckin broad will never', 'green paper gold teeth and pregnant', 'guidance that i had is splattered', 'he said hes sick of the', 'heres the number to my therapist', 'hes fuckin awesome with listenin haha', 'hes not fuckin workin i think', 'him all your problems hes fuckin', 'him to quit bitchin this isnt', 'his bitches in my bedroom hey', 'home for some damn adventure time', 'how about i start a team', 'i beat deshay up with the', 'i dont give a fuck so', 'i got somethin to feed em', 'i had is splattered on cement', 'i just wanna boogie to some', 'i just wanna know if my', 'i slipped myself some pink xannies', 'i start a team of leaders', 'i think im wastin my damn', 'i told him to quit bitchin', 'i want fuck money diamonds and', 'i went home for some damn', 'if my father would ever like', 'ill crash that fuckin airplane that', 'im a fuckin walkin paradox no', 'im an overachiever so how about', 'im clockin three past six and', 'im not gay i just wanna', 'im stabbin any bloggin faggot hipster', 'im wolf ace gon put that', 'im wolf that was me who', 'im wolf tyler put this fuckin', 'ima scribble this sin and shit', 'is tellin me that shes been', 'it bunnyhopped off my shoulder now', 'its been a couple months', 'its some cooking books the black', 'jesus called he said hes sick', 'just wanna boogie to some marvin', 'just wanna know if my father', 'kids never wanted to read em', 'know if my father would ever', 'louder than words let me try', 'making crack rocks outta pussy nigga', 'man fuck the fame and all', 'me that shes been getting intimate', 'me who shoved a cock in', 'moms gone that fuckin broad will', 'money diamonds and bitches dont need', 'my father would ever like me', 'my moms gone that fuckin broad', 'my shoulder now my conscience dead', 'nigga jasper tryin to get grown', 'no v tech shit or columbine', 'not again another critic writing report', 'not fuckin workin i think im', 'not gay i just wanna boogie', 'now the only guidance that i', 'of his bitches in my bedroom', 'of the dicks thats nine cocks', 'off my shoulder now my conscience', 'oh not again another critic writing', 'ones at i got somethin to', 'only guidance that i had is', 'overachiever so how about i start', 'paper gold teeth and pregnant golden', 'pick up stevie wonder to be', 'problems hes fuckin awesome with listenin', 'put that fuckin hole in my', 'put this fuckin knife in my', 'quit bitchin this isnt a fuckin', 'rappin as im mockin deaf rock', 'revenge of the dicks thats nine', 'rocks outta pussy nigga fishbones haha', 'said hes sick of the disses', 'say success is the best revenge', 'seven of his bitches in my', 'shes been getting intimate with men', 'shoved a cock in your bitch', 'shrink sheesh i already got mine', 'slipped myself some pink xannies yeah', 'snap back green chchchia fuckin leaves', 'so hes probably just like me', 'so how about i start a', 'so i beat deshay up with', 'some cooking books the black kids', 'speak louder than words let me', 'stab bruno mars in his goddamn', 'stabbin any bloggin faggot hipster with', 'stevie wonder to be the wide', 'still aint perm her fuckin weave', 'still suicidal i am', 'stop until the cops come in', 'swallow the cinnamon ima scribble this', 'syd is tellin me that shes', 'syd shut the fuck up', 'synthetic wigs made of anwars dreadlocks', 'tell him all your problems hes', 'tellin me that shes been getting', 'than words let me try this', 'that faggot nigga bob is in', 'that fuckin airplane that that faggot', 'that fuckin broad will never understand', 'that fuckin hole in my head', 'that i had is splattered on', 'that shes been getting intimate with', 'that that faggot nigga bob is', 'that was me who shoved a', 'thats nine cocks that cock s', 'the black kids never wanted to', 'the cinnamon ima scribble this sin', 'the dicks thats nine cocks that', 'the fame and all the hype', 'the fat ones at i got', 'the fuck man fuck the fame', 'the house in allover print panties', 'the number to my therapist shit', 'the only guidance that i had', 'the revenge of the dicks thats', 'the stack of magazines im in', 'then it bunnyhopped off my shoulder', 'they say success is the best', 'think im wastin my damn time', 'this aint no v tech shit', 'this fuckin knife in my hand', 'this nigga jasper tryin to get', 'this the revenge of the dicks', 'three past six and goin postal', 'threesomes with a fuckin triceratops reptar', 'tina still aint perm her fuckin', 'to be the wide receiver cool', 'to quit bitchin this isnt a', 'told him to quit bitchin this', 'tyler put this fuckin knife in', 'uh wolf haley golf fuckin wang', 'uh wolf haley golf wang yeah', 'uh wolf haley uh golf wang', 'up stevie wonder to be the', 'up with the stack of magazines', 'wanna know if my father would', 'want fuck money diamonds and bitches', 'was me who shoved a cock', 'wearin synthetic wigs made of anwars', 'went home for some damn adventure', 'what the fuck man fuck the', 'what you think of hayley williams', 'whatd you do i slipped myself', 'where the fat ones at i', 'while syd is tellin me that', 'who shoved a cock in your', 'with the stack of magazines im', 'wolf ace gon put that fuckin', 'wolf haley uh golf wang', 'wolf that was me who shoved', 'wolf tyler put this fuckin knife', 'wonder to be the wide receiver', 'wont stop until the cops come', 'words let me try this shit', 'workin i think im wastin my', 'you do i slipped myself some', 'you tell him all your problems', 'your problems hes fuckin awesome with']\n",
            "Initializing model...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'goblin the in my my em would time wang wang wang wang wang'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approach 2: XLNET"
      ],
      "metadata": {
        "id": "iXFdBfgtLOjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = open(\"/content/drive/MyDrive/teardrops on my guitar.txt\", \"r\", encoding = \"utf8\")\n",
        "test_input = test_input.read()\n",
        "test_input = test_input.replace(\"\\n\\n\", \"\\n\")\n",
        "test_input = test_input.replace(\"\\n\\n\", \"\\n\")\n",
        "#print(test_input)"
      ],
      "metadata": {
        "id": "IAUbabg3r1o3"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Credits: https://huggingface.co/docs/transformers/model_doc/xlnet; https://towardsdatascience.com/build-a-bidirectional-text-generator-with-xlnet-49d9d37b48a9\n",
        "\n",
        "# Initializing a XLNet configuration\n",
        "configuration = XLNetConfig()\n",
        "\n",
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
        "model = XLNetLMHeadModel.from_pretrained('xlnet-large-cased')\n",
        "\n",
        "# Padding text to help Transformer-XL and XLNet with short prompts\n",
        "# in https://github.com/rusiaaman/XLNet-gen#methodology\n",
        "# and https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e\n",
        "PADDING_TEXT = test_input\n",
        "\n",
        "torch.manual_seed(0)\n",
        "# We show how to setup inputs to predict a next token using a bi-directional context.\n",
        "# We will predict masked tokens\n",
        "input_ids = torch.tensor(tokenizer.encode(PADDING_TEXT + \"I gave you three apples. <mask> have <mask> apples in hands\", add_special_tokens=False)).unsqueeze(0)  \n",
        "\n",
        "targets = [ -6, -4]\n",
        "\n",
        "perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
        "perm_mask[0, :, targets] = 1.0  # Previous tokens don't see last token\n",
        "\n",
        "target_mapping = torch.zeros((1, len(targets), input_ids.shape[1]), dtype=torch.float)  \n",
        "\n",
        "target_mapping[0, 0, targets[0]] = 1.0  # Our first  prediction \n",
        "target_mapping[0, 1, targets[1]] = 1.0  # Our second  prediction \n",
        "\n",
        "input_ids_tensor = input_ids.to(\"cuda\")\n",
        "target_mapping_tensor = target_mapping.to(\"cuda\")\n",
        "perm_mask_tensor = perm_mask.to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "if torch.cuda.is_available(): model.to('cuda') #if we have a GPU \n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs = model(input_ids_tensor, perm_mask=perm_mask_tensor, target_mapping=target_mapping_tensor)\n",
        "next_token_logits = outputs[0]  # Output has shape [target_mapping.size(0), target_mapping.size(1), config.vocab_size]\n",
        "\n",
        "for j in range(len(targets)):\n",
        "  predicted_k_indexes = torch.topk(outputs[0][0][j],k=5)\n",
        "  predicted_logits_list = predicted_k_indexes[0] \n",
        "  predicted_indexes_list = predicted_k_indexes[1] \n",
        "    \n",
        "  print (\"predicted word:\",tokenizer.decode(input_ids[0][targets[j]].item()), j)\n",
        "  for i,item  in enumerate(predicted_indexes_list):\n",
        "      the_index = predicted_indexes_list[i].item()\n",
        "      print(\"word and logits\",tokenizer.decode(the_index),predicted_logits_list[i].item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ64vp19MdMU",
        "outputId": "52087ca4-9bcc-4e98-ea71-bf3058a20455"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicted word: <mask> 0\n",
            "word and logits I -17.119409561157227\n",
            "word and logits You -18.564960479736328\n",
            "word and logits I -20.358469009399414\n",
            "word and logits We -20.713825225830078\n",
            "word and logits You -21.69858741760254\n",
            "predicted word: <mask> 1\n",
            "word and logits three -19.076858520507812\n",
            "word and logits the -21.324016571044922\n",
            "word and logits two -22.618837356567383\n",
            "word and logits your -22.90503692626953\n",
            "word and logits my -23.162578582763672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to select topK tokens from the probability list and \n",
        "# then based on the selected K word distribution get sample of random token IDs\n",
        "def choose_from_top(probs, k=5, sample_size=1):\n",
        "    ind = np.argpartition(probs, -k)[-k:]\n",
        "    top_prob = probs[ind]\n",
        "    # print(tokenizer.decode(ind))\n",
        "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
        "    choice = np.random.choice(k, sample_size, p = top_prob, replace=False)\n",
        "    token_ids = ind[choice]\n",
        "    return token_ids\n",
        "\n",
        "sent = \"So I drive home alone\"\n",
        "topk = 10\n",
        "n = 20\n",
        "# Lower temperatures make the model more confident in its top choices, while temperatures greater than 1 decrease confidence.\n",
        "temperature = 5\n",
        "\n",
        "model.eval()\n",
        "if torch.cuda.is_available(): model.to('cuda') #if we have a GPU \n",
        "\n",
        "sent_tokens = tokenizer.encode(sent, add_special_tokens=False)\n",
        "mask_tokens = tokenizer.encode('<mask>', add_special_tokens=False)\n",
        "padding_tokens = tokenizer.encode(PADDING_TEXT, add_special_tokens=False)\n",
        "   \n",
        "for i in range(n):\n",
        "  input = mask_tokens + sent_tokens + mask_tokens     \n",
        "  target_id1 = -len(input)\n",
        "  target_id2 = -1\n",
        "\n",
        "  input_ids = torch.tensor(padding_tokens + input).unsqueeze(0)   # We will predict masked tokens\n",
        "\n",
        "  perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
        "  perm_mask[0, :, [target_id1, target_id2]] = 1.0  # Previous tokens don't see last token\n",
        "\n",
        "  target_mapping = torch.zeros((1, 2, input_ids.shape[1]), dtype=torch.float)  \n",
        "  target_mapping[0, 0, target_id1] = 1.0  # Our first  prediction \n",
        "  target_mapping[0, 1, target_id2] = 1.0  # Our second  prediction \n",
        "\n",
        "  input_ids_tensor = input_ids.to(\"cuda\")\n",
        "  target_mapping_tensor = target_mapping.to(\"cuda\")\n",
        "  perm_mask_tensor = perm_mask.to(\"cuda\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_ids_tensor, perm_mask=perm_mask_tensor, target_mapping=target_mapping_tensor)\n",
        "\n",
        "  predicted_tokens = []\n",
        "  \n",
        "  for j in range(2):\n",
        "    probs = torch.nn.functional.softmax(outputs[0][0][j]/temperature, dim = 0).to('cpu').numpy()\n",
        "    predicted_tokens.append(choose_from_top(probs, k=topk, sample_size=1))\n",
        "\n",
        "  if i % 2 == 0:    \n",
        "    tok = predicted_tokens[0][0]\n",
        "    sent_tokens = [tok] + sent_tokens \n",
        "    print('left: ', tokenizer.decode(sent_tokens))\n",
        "  else:     \n",
        "    tok = predicted_tokens[1][0]\n",
        "    sent_tokens = sent_tokens + [tok]\n",
        "    print(\"right: \", tokenizer.decode(sent_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCne0ujLmYNW",
        "outputId": "e613eeb2-7d7b-4621-bdd1-cf703ddeeb45"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "left:  it So I drive home alone\n",
            "right:  it So I drive home alone\n",
            "left:  through it So I drive home alone\n",
            "right:  through it So I drive home alone -\n",
            "left:  my through it So I drive home alone -\n",
            "right:  my through it So I drive home alone - a\n",
            "left:  I my through it So I drive home alone - a\n",
            "right:  I my through it So I drive home alone - a song\n",
            "left:  That I my through it So I drive home alone - a song\n",
            "right:  That I my through it So I drive home alone - a song of\n",
            "left:  See That I my through it So I drive home alone - a song of\n",
            "right:  See That I my through it So I drive home alone - a song of hope\n",
            "left:  me See That I my through it So I drive home alone - a song of hope\n",
            "right:  me See That I my through it So I drive home alone - a song of hope A\n",
            "left:  Behind me See That I my through it So I drive home alone - a song of hope A\n",
            "right:  Behind me See That I my through it So I drive home alone - a song of hope A cry\n",
            "left:  Looking Behind me See That I my through it So I drive home alone - a song of hope A cry\n",
            "right:  Looking Behind me See That I my through it So I drive home alone - a song of hope A cry of\n",
            "left:  But Looking Behind me See That I my through it So I drive home alone - a song of hope A cry of\n",
            "right:  But Looking Behind me See That I my through it So I drive home alone - a song of hope A cry of anguish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a combination of beam and top-k generation to generate sequences of n tokens from both sides \n",
        "\n",
        "import random\n",
        "\n",
        "padding_tokens = tokenizer.encode(PADDING_TEXT, add_special_tokens=False)\n",
        "mask_tokens = tokenizer.encode('<mask>', add_special_tokens=False)\n",
        "\n",
        "model.eval()\n",
        "if torch.cuda.is_available(): model.to('cuda') #if we have a GPU \n",
        "\n",
        "def candidates_gen(sent_tokens, candidate=([], 1, []), d='left', n_candidates=5, topk=20, temperature=5):\n",
        "  branch_candidates = []  \n",
        "  cand_tokens = candidate[0]\n",
        "  \n",
        "  if d == 'right':    \n",
        "    input = sent_tokens + cand_tokens + mask_tokens     \n",
        "    \n",
        "    target_id = -1\n",
        "    input_ids = torch.tensor(padding_tokens + input).unsqueeze(0)  \n",
        "\n",
        "    perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
        "    perm_mask[0, :, target_id] = 1.0  # Previous tokens don't see last token\n",
        "  else:        \n",
        "    input = mask_tokens + cand_tokens + sent_tokens    \n",
        "    \n",
        "    target_id = -len(input)  \n",
        "    input_ids = torch.tensor(padding_tokens + input).unsqueeze(0)  \n",
        "\n",
        "    perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
        "    perm_mask[0, :, [target_id - i for i in range(100)]] = 1.0  # Mask additional previos tokens to improve left-side generation\n",
        "\n",
        "  # We will predict masked tokens \n",
        "  target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float)  \n",
        "  target_mapping[0, 0, target_id] = 1.0  # Our right  prediction \n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    input_ids_tensor = input_ids.to(\"cuda\")\n",
        "    target_mapping_tensor = target_mapping.to(\"cuda\")\n",
        "    perm_mask_tensor = perm_mask.to(\"cuda\")\n",
        "  else:\n",
        "    input_ids_tensor = input_ids\n",
        "    target_mapping_tensor = target_mapping\n",
        "    perm_mask_tensor = perm_mask\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_ids_tensor, perm_mask=perm_mask_tensor, target_mapping=target_mapping_tensor)\n",
        "\n",
        "  probs = torch.nn.functional.softmax(outputs[0][0][0]/temperature, dim = 0)\n",
        "  selected_indexes = choose_from_top(probs.to('cpu').numpy(), k=topk, sample_size=n_candidates)\n",
        "  selected_probs = probs[selected_indexes]\n",
        "\n",
        "  for i,item  in enumerate(selected_indexes):\n",
        "      the_index = item.item()\n",
        "      if d == \"right\":\n",
        "        new_sent = cand_tokens + [the_index]\n",
        "      elif d == \"left\":\n",
        "        new_sent = [the_index] + cand_tokens\n",
        "      \n",
        "      prob = selected_probs[i].item()\n",
        "      # add word combinations to branch_candidates in format [sentence, cumulative probability, all probs]\n",
        "      branch_candidates.append((new_sent, candidate[1] * prob, candidate[2] + [prob]))\n",
        "  \n",
        "  return branch_candidates\n",
        "\n",
        "def beam_gen(sent_tokens, candidates, depth=5, d='right', sample_size=2, topk=10, temperature=5):\n",
        "  beams = candidates[:]\n",
        "  new_candidates = candidates[:]\n",
        "  while depth > 0:\n",
        "    new_candidates = []\n",
        "    for candidate in candidates:\n",
        "      for new_candidate in candidates_gen(sent_tokens, candidate, d, sample_size, topk, temperature):\n",
        "        beams.append(new_candidate)\n",
        "        new_candidates.append(new_candidate)   \n",
        "    print(\"Number of beams:\", len(new_candidates))    \n",
        "    candidates = new_candidates[:]\n",
        "    depth -= 1\n",
        "  sorted_beams = sorted(new_candidates, key=lambda tup: np.sum(np.log10(tup[2])), reverse=True)\n",
        "  return beams, sorted_beams\n",
        "\n",
        "def bi_generator(sent, direction, first_sample_size, sample_size, n_tokens, topk, iterations, temperature):\n",
        "  sent_tokens = tokenizer.encode(sent, add_special_tokens=False) \n",
        "\n",
        "  for i in range(iterations):\n",
        "    if (i % 2 == 0 and direction == 'both') or direction == 'left':\n",
        "      print('>> left side generation')\n",
        "      candidates = candidates_gen(sent_tokens=sent_tokens, d='left', n_candidates=first_sample_size,  topk=topk, temperature=temperature)\n",
        "      beams, sorted_beams = beam_gen(sent_tokens, candidates, n_tokens-1, 'left', sample_size, topk, temperature=temperature)\n",
        "      topn = len(sorted_beams)//5 if len(sorted_beams) > 4 else len(sorted_beams)\n",
        "      selected_candidate = random.choice(sorted_beams[:topn])\n",
        "      sent_tokens = selected_candidate[0] + sent_tokens\n",
        "      print(tokenizer.decode(sent_tokens))\n",
        "    if (i % 2 != 0 and direction == 'both') or direction == 'right':\n",
        "      print('>> right side generation')\n",
        "      candidates = candidates_gen(sent_tokens=sent_tokens, d='right', n_candidates=first_sample_size, topk=topk, temperature=temperature)\n",
        "      beams, sorted_beams = beam_gen(sent_tokens, candidates, n_tokens-1, 'right', sample_size, topk, temperature=temperature)\n",
        "      topn = len(sorted_beams)//5 if len(sorted_beams) > 4 else len(sorted_beams)\n",
        "      selected_candidate = random.choice(sorted_beams[:topn])\n",
        "      sent_tokens = sent_tokens + selected_candidate[0]\n",
        "      print(tokenizer.decode(sent_tokens))\n",
        "    \n",
        "  return tokenizer.decode(sent_tokens)\n",
        "\n",
        "sent = \"Jesus called, he said he's sick of the disses\"  \n",
        "first_sample_size = 4\n",
        "sample_size = 2\n",
        "n_tokens = 4\n",
        "topk = 20\n",
        "iterations = 6\n",
        "temperature = 4\n",
        "direction = \"both\"\n",
        "\n",
        "bi_generator(sent, direction, first_sample_size, sample_size, n_tokens, topk, iterations, temperature);\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "budZ8DEgt5IN",
        "outputId": "4efe5b5a-923b-4c94-8b41-9465f8f4a512"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> left side generation\n",
            "Number of beams: 8\n",
            "Number of beams: 16\n",
            "Number of beams: 32\n",
            "'re dead But Jesus called, he said he's sick of the disses\n",
            ">> right side generation\n",
            "Number of beams: 8\n",
            "Number of beams: 16\n",
            "Number of beams: 32\n",
            "'re dead But Jesus called, he said he's sick of the disses What do he have\n",
            ">> left side generation\n",
            "Number of beams: 8\n",
            "Number of beams: 16\n",
            "Number of beams: 32\n",
            "we all know we're dead But Jesus called, he said he's sick of the disses What do he have\n",
            ">> right side generation\n",
            "Number of beams: 8\n",
            "Number of beams: 16\n",
            "Number of beams: 32\n",
            "we all know we're dead But Jesus called, he said he's sick of the disses What do he have no right to ask\n",
            ">> left side generation\n",
            "Number of beams: 8\n",
            "Number of beams: 16\n",
            "Number of beams: 32\n",
            "me tacause we all know we're dead But Jesus called, he said he's sick of the disses What do he have no right to ask\n",
            ">> right side generation\n",
            "Number of beams: 8\n",
            "Number of beams: 16\n",
            "Number of beams: 32\n",
            "me tacause we all know we're dead But Jesus called, he said he's sick of the disses What do he have no right to ask What is fuckin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approach 3: GPT-2"
      ],
      "metadata": {
        "id": "G9f-wBnIcRfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging, sys\n",
        "logging.disable(sys.maxsize)\n",
        "from huggingface_hub.hf_api import HfApi"
      ],
      "metadata": {
        "id": "OB2Z23s5iXGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Codebase Credits: https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb#scrollTo=rfyl8xdLz8ZW\n",
        "\n",
        "artist_name = \"Adele\"\n",
        "check_dataset = True\n",
        "num_train_epochs =  1\n",
        "\n",
        "hfapi = HfApi()\n",
        "user, namespace = 'huggingartists-app', 'huggingartists'\n",
        "token = hfapi.login(user, namespace)\n",
        "assert hfapi.whoami(token)['name'] == user, \"Could not log into Hugging Face\"\n",
        "!mkdir /root/.huggingface -p\n",
        "text_file = open(\"/root/.huggingface/token\", \"w+\")\n",
        "text_file.write(token)\n",
        "text_file.close() \n",
        "\n",
        "TOKEN = \"qp1ggvlvBI8VJmNL_rfvqHUugm5QTvTGssch_3C1FUV6XpvRIYJifhAnIHwyPpwK\"\n",
        "DATASET_LOAD_SCRIPT_URL = \"https://raw.githubusercontent.com/AlekseyKorshuk/huggingartists/main/datasets/dataset.py\"\n",
        "DATASET_CARD_URL = \"https://raw.githubusercontent.com/AlekseyKorshuk/huggingartists/main/datasets/README.md\"\n",
        "MODEL_CARD_URL = \"https://raw.githubusercontent.com/AlekseyKorshuk/huggingartists/main/models/README.md\"\n",
        "\n",
        "from IPython.display import display, HTML, Javascript, clear_output\n",
        "import lyricsgenius\n",
        "from tqdm.notebook import tqdm as bar\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from datasets import Dataset, DatasetDict\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "import langdetect\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "import wandb\n",
        "import pathlib\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import asyncio\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import sys\n",
        "sys.setrecursionlimit(99999)\n",
        "import aiohttp\n",
        "\n",
        "parser = requests.get(\"https://raw.githubusercontent.com/AlekseyKorshuk/huggingartists/main/datasets/parse.py\").text\n",
        "with open('parse.py', 'w+') as f:\n",
        "  f.write(parser)\n",
        "\n",
        "genius = lyricsgenius.Genius(TOKEN)\n",
        "artist = genius.search_artist(artist_name, max_songs=0, get_full_info=False)\n",
        "\n",
        "\n",
        "from IPython.display import clear_output \n",
        "clear_output()\n",
        "from IPython.utils import io\n",
        "\n",
        "\n",
        "from IPython.display import display, HTML, Javascript, clear_output\n",
        "\n",
        "\n",
        "def stylize():\n",
        "    \"Handle dark mode\"\n",
        "    display(HTML('''\n",
        "    <style>\n",
        "    :root {\n",
        "        --table_bg: #EBF8FF;\n",
        "    }\n",
        "    html[theme=dark] {\n",
        "        --colab-primary-text-color: #d5d5d5;\n",
        "        --table_bg: #2A4365;\n",
        "    }\n",
        "    .jupyter-widgets {\n",
        "        color: var(--colab-primary-text-color);\n",
        "    }\n",
        "    table {\n",
        "        border-collapse: collapse !important;\n",
        "    }\n",
        "    td {\n",
        "        text-align:left !important;\n",
        "        border: solid var(--table_bg) !important;\n",
        "        border-width: 1px 0 !important;\n",
        "        padding: 6px !important;\n",
        "    }\n",
        "    tr:nth-child(even) {\n",
        "        background-color: var(--table_bg) !important;\n",
        "    }\n",
        "    .table_odd {\n",
        "        background-color: var(--table_bg) !important;\n",
        "        margin: 0 !important;\n",
        "    }\n",
        "    .table_even {\n",
        "        border: solid var(--table_bg) !important;\n",
        "        border-width: 1px 0 !important;\n",
        "        margin: 0 !important;\n",
        "    }\n",
        "    .jupyter-widgets {\n",
        "        margin: 6px;\n",
        "    }\n",
        "    .widget-html-content {\n",
        "        font-size: var(--colab-chrome-font-size) !important;\n",
        "        line-height: 1.24 !important;\n",
        "    }\n",
        "    </style>'''))\n",
        "\n",
        "\n",
        "def create_dataset_load_script(model_name):\n",
        "  response = requests.get(DATASET_LOAD_SCRIPT_URL)\n",
        "  text = str(response.text)\n",
        "  text = text.replace(\"MODEL_NAME\", model_name)\n",
        "  with open(f'{model_name}/{model_name}.py', 'w+') as f:\n",
        "    f.write(text)\n",
        "\n",
        "def create_dataset_card(model_name, settings):\n",
        "  response = requests.get(DATASET_CARD_URL)\n",
        "  text = str(response.text)\n",
        "  for key in settings.keys():\n",
        "    text = text.replace(key, str(settings[key]))\n",
        "  with open(f'{model_name}/README.md', 'w+') as f:\n",
        "    f.write(text)\n",
        "\n",
        "def create_model_card(model_name, settings):\n",
        "  response = requests.get(MODEL_CARD_URL)\n",
        "  text = str(response.text)\n",
        "  for key in settings.keys():\n",
        "    text = text.replace(key, str(settings[key]))\n",
        "  with open(f'{model_name}/README.md', 'w+') as f:\n",
        "    f.write(text)\n",
        "\n",
        "def artist_songs(artist_id, per_page=50, page=None, sort='popularity'):\n",
        "  url = f'https://api.genius.com/artists/{artist_id}/songs?sort={sort}&per_page={per_page}&page={page}'\n",
        "  headers = {\n",
        "      'Authorization': f'Bearer {TOKEN}'\n",
        "  }\n",
        "  data = requests.get(\n",
        "      url,\n",
        "      headers=headers, \n",
        "      stream=True\n",
        "  ).json()\n",
        "  return data['response']\n",
        "\n",
        "\n",
        "def get_artist_song_urls(artist_id):\n",
        "  \n",
        "  urls = []\n",
        "  next_page = 1\n",
        "  with bar(total=None) as pbar:\n",
        "    pbar.set_description(\"⏳ Searching songs\")\n",
        "    while next_page is not None:\n",
        "\n",
        "      data = artist_songs(artist.id, per_page=50, page=next_page)\n",
        "      next_page = data['next_page']\n",
        "      \n",
        "      for song in data['songs']:\n",
        "        urls.append(song['url'])\n",
        "      pbar.update(len(data['songs']))   \n",
        "\n",
        "    pbar.set_description(\"✅ Done\")\n",
        "  return urls\n",
        "\n",
        "async def get_song_urls(artist_id):\n",
        "  access_token = 'Bearer ' + TOKEN\n",
        "  authorization_header = {'authorization': access_token}\n",
        "  urls = []\n",
        "  async with aiohttp.ClientSession(headers=authorization_header) as session:\n",
        "    with bar(total=None) as pbar:\n",
        "      pbar.set_description(\"⏳ Searching songs...\")\n",
        "      next_page = 1\n",
        "      while next_page is not None:\n",
        "        async with session.get(f\"https://api.genius.com/artists/{artist_id}/songs?sort=popularity&per_page=50&page={next_page}\", timeout=999) as resp:\n",
        "          response = await resp.json()\n",
        "          response = response['response']\n",
        "        next_page = response['next_page']\n",
        "        \n",
        "        for song in response['songs']:\n",
        "          urls.append(song['url'])\n",
        "        pbar.update(len(response['songs']))\n",
        "      pbar.set_description(\"✅ Done\")\n",
        "  return urls\n",
        "\n",
        "\n",
        "def _get_lyrics(song_url):\n",
        "    text = requests.get(song_url, stream=True).text\n",
        "    \n",
        "    html = BeautifulSoup(text.replace('<br/>', '\\n'), 'html.parser')\n",
        "    div = html.find(\"div\", class_=re.compile(\"^lyrics$|Lyrics__Root\"))\n",
        "    if div is None:\n",
        "      return None\n",
        "\n",
        "    lyrics = div.get_text()\n",
        "\n",
        "    lyrics = re.sub(r'(\\[.*?\\])*', '', lyrics)\n",
        "    lyrics = re.sub('\\n{2}', '\\n', lyrics)  # Gaps between verses\n",
        "    \n",
        "    lyrics = str(lyrics.strip(\"\\n\"))\n",
        "    lyrics = lyrics.replace(\"EmbedShare URLCopyEmbedCopy\", \"\").replace(\"'\", \"\")\n",
        "    lyrics = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", lyrics)\n",
        "    lyrics = re.sub(r'\\d+$', '', lyrics)\n",
        "    lyrics = str(lyrics).lstrip().rstrip()\n",
        "    lyrics = str(lyrics).replace(\"\\n\\n\", \"\\n\")\n",
        "    lyrics = str(lyrics).replace(\"\\n\\n\", \"\\n\")\n",
        "    lyrics = re.sub(' +', ' ', lyrics)\n",
        "    lyrics = str(lyrics).replace('\"', \"\")\n",
        "    lyrics = str(lyrics).replace(\"'\", \"\")\n",
        "    lyrics = str(lyrics).replace(\"*\", \"\")\n",
        "    return str(lyrics)\n",
        "\n",
        "\n",
        "def get_lyrics(url):\n",
        "  return _get_lyrics(url)\n",
        "\n",
        "\n",
        "def process_page(html):\n",
        "    '''Meant for CPU-bound workload'''\n",
        "    html = BeautifulSoup(html.replace('<br/>', '\\n'), 'html.parser')\n",
        "    div = html.find(\"div\", class_=re.compile(\"^lyrics$|Lyrics__Root\"))\n",
        "    if div is None:\n",
        "      lyrics = \"\"\n",
        "    else:\n",
        "      lyrics = div.get_text()\n",
        "    \n",
        "    lyrics = re.sub(r'(\\[.*?\\])*', '', lyrics)\n",
        "    lyrics = re.sub('\\n{2}', '\\n', lyrics)  # Gaps between verses\n",
        "    \n",
        "    lyrics = str(lyrics.strip(\"\\n\"))\n",
        "    lyrics = lyrics.replace(\"EmbedShare URLCopyEmbedCopy\", \"\").replace(\"'\", \"\")\n",
        "    lyrics = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", lyrics)\n",
        "    lyrics = re.sub(r'\\d+$', '', lyrics)\n",
        "    lyrics = str(lyrics).lstrip().rstrip()\n",
        "    lyrics = str(lyrics).replace(\"\\n\\n\", \"\\n\")\n",
        "    lyrics = str(lyrics).replace(\"\\n\\n\", \"\\n\")\n",
        "    lyrics = re.sub(' +', ' ', lyrics)\n",
        "\n",
        "    lyrics = str(lyrics).replace('\"', \"\")\n",
        "    # lyrics = str(lyrics).replace(\"'\", \"\")\n",
        "    lyrics = str(lyrics).replace(\"*\", \"\")\n",
        "    return lyrics #, re.compile(\"^lyrics$|Lyrics__Root\")\n",
        "\n",
        "\n",
        "async def fetch_page(url, session):\n",
        "    '''Meant for IO-bound workload'''\n",
        "    async with session.get(url, timeout=999) as res:\n",
        "      return await res.text()\n",
        "\n",
        "\n",
        "async def process(url, session, pool, pbar):\n",
        "    html = await fetch_page(url, session)\n",
        "    pbar.update(1)\n",
        "    return await asyncio.wrap_future(pool.submit(process_page, html))\n",
        "\n",
        "\n",
        "async def dispatch(urls, pbar):\n",
        "    print('\\n')\n",
        "    pool = ProcessPoolExecutor()\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        coros = (process(url, session, pool, pbar) for url in urls)\n",
        "        lyrics = await asyncio.gather(*coros)\n",
        "    return lyrics\n",
        "\n",
        "def create_dataset(lyrics):\n",
        "  train_percentage = 0.9\n",
        "  validation_percentage = 0.07\n",
        "  test_percentage = 0.03\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  dataset['train'] = Dataset.from_dict({'text': list(lyrics)})\n",
        "\n",
        "  # train, validation, test = np.split(lyrics, [int(len(lyrics)*train_percentage), int(len(lyrics)*(train_percentage + validation_percentage))])\n",
        "  # if len(list(train)) != 0:\n",
        "  #   dataset['train'] = Dataset.from_dict({'text': list(train)})\n",
        "  # if len(list(validation)) != 0:\n",
        "  #   dataset['validation'] = Dataset.from_dict({'text': list(validation)})\n",
        "  # if len(list(test)) != 0:\n",
        "  #   dataset['test'] = Dataset.from_dict({'text': list(test)})\n",
        "  # del train\n",
        "  # del validation\n",
        "  # del test\n",
        "\n",
        "  datasets = DatasetDict(dataset)\n",
        "  del dataset\n",
        "  return datasets\n",
        "\n",
        "\n",
        "def commit_files(model_name, message):\n",
        "  %cd $model_name\n",
        "  !git add .\n",
        "  !git commit -m \"{message}\"\n",
        "  !git push\n",
        "  %cd ..\n",
        "\n",
        "def parse_dataset(model_name, namespace, artist_id):\n",
        "  \n",
        "  with io.capture_output() as captured:\n",
        "    url = f\"https://huggingface.co/datasets/{namespace}/{model_name}/tree/main\"\n",
        "    data = requests.get(url).text\n",
        "    if data == \"Not Found\":\n",
        "      !huggingface-cli repo create $model_name --type dataset --organization $namespace -y\n",
        "    !rm -rf $model_name\n",
        "    !git clone https://$user:$token@huggingface.co/datasets/$namespace/$model_name\n",
        "    \n",
        "  save_path = f\"{model_name}/datasets.json\"\n",
        "  !python parse.py \\\n",
        "      --artist_id=$artist_id \\\n",
        "      --token=$TOKEN \\\n",
        "      --save_path=$save_path\n",
        "  \n",
        "  with io.capture_output() as captured:\n",
        "    %cd $model_name\n",
        "    # !git lfs untrack \"*.json\"\n",
        "    !git lfs track \"*.json\"\n",
        "    # !rm -rf\n",
        "    %cd ..\n",
        "    # !rm -rf $model_name\n",
        "    # !mkdir $model_name\n",
        "    create_dataset_load_script(model_name)\n",
        "    global artist_url\n",
        "    root_directory = Path(model_name)\n",
        "    size = sum(f.stat().st_size for f in root_directory.glob('**/*') if f.is_file()) / 1000000\n",
        "    \n",
        "    with open(save_path) as f:\n",
        "      data = json.load(f)\n",
        "    \n",
        "    settings = {\n",
        "        'LANGUAGE': 'en',\n",
        "        'USER_HANDLE': model_name,\n",
        "        'YEAR': datetime.datetime.now().year,\n",
        "        'USER_NAME': artist.name,\n",
        "        'USER_PROFILE': artist.image_url,\n",
        "        'TRAIN_SIZE': len(data['train']),\n",
        "        'SIZE': str(size)\n",
        "    }\n",
        "    create_dataset_card(model_name, settings)\n",
        "    commit_files(model_name, namespace)\n",
        "    !rm -rf $model_name\n",
        "\n",
        "from datasets import ClassLabel\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "\n",
        "def show_random_elements(dataset, num_examples=3):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "    print(dataset[picks]['text'])\n",
        "    print(len(dataset[picks]['text']))\n",
        "    df = pd.DataFrame(dataset[picks]['text'])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(df.to_html()))\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"])\n",
        "\n",
        "\n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "        # customize this part to your needs.\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "\n",
        "def show_result(dataset, num_examples=3):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    data = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "        data.append(str(dataset[pick]))  \n",
        "    df = pd.DataFrame(data)\n",
        "    # for column, typ in dataset.features.items():\n",
        "    #     if isinstance(typ, ClassLabel):\n",
        "    #         df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(df.to_html(index=False).replace(\"\\\\n\",\"<br>\")))\n",
        "\n",
        "\n",
        "def post_process(output_sequences):\n",
        "    predictions = []\n",
        "    generated_sequences = []\n",
        "\n",
        "    max_repeat = 2\n",
        "\n",
        "    # decode prediction\n",
        "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "        generated_sequence = generated_sequence.tolist()\n",
        "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True, skip_special_tokens=True)\n",
        "        generated_sequences.append(text.strip())\n",
        "                    \n",
        "    for i, g in enumerate(generated_sequences):\n",
        "        res = str(g).replace('\\n\\n\\n', '\\n').replace('\\n\\n', '\\n')\n",
        "        lines = res.split('\\n')\n",
        "        # print(lines)\n",
        "        i = max_repeat\n",
        "        while i != len(lines):\n",
        "          remove_count = 0\n",
        "          for index in range(0, max_repeat):\n",
        "            # print(i - index - 1, i - index)\n",
        "            if lines[i - index - 1] == lines[i - index]:\n",
        "              remove_count += 1\n",
        "          if remove_count == max_repeat:\n",
        "            lines.pop(i)\n",
        "            i -= 1\n",
        "          else:\n",
        "            i += 1\n",
        "        predictions.append('\\n'.join(lines))\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def get_table(table_data):\n",
        "  html = (\"</head>\\r\\n\"\n",
        "    \"<body>\\r\\n\\r\\n\"\n",
        "    \"<h2></h2>\"\n",
        "    \"\\r\\n\\r\\n\"\n",
        "    \"<table>\\r\\n\"\n",
        "    \"    <colgroup>\\r\\n\"\n",
        "    \"       <col span=\\\"1\"\n",
        "    \"\\\" style=\\\"width: 10\"\n",
        "    \"%;\\\">\\r\\n\"\n",
        "    \"       <col span=\\\"1\"\n",
        "    \"\\\" style=\\\"width: 10\"\n",
        "    \"0%;\\\">\\r\\n\"\n",
        "    \"    </colgroup>\\r\\n\"\n",
        "    f\"{' '.join(table_data)}\"\n",
        "    \"</table>\\r\\n\\r\\n\"\n",
        "    \"</body>\\r\\n\"\n",
        "    \"</html>\")\n",
        "  \n",
        "  return html\n",
        "\n",
        "def get_share_button(url):\n",
        "    return f'''\n",
        "            <div style=\"width: 76px;\">\n",
        "                <a target=\"_blank\" href=\"{url}\" style='background-color:rgb(27, 149, 224);border-bottom-left-radius:4px;border-bottom-right-radius:4px;border-top-left-radius:4px;border-top-right-radius:4px;box-sizing:border-box;color:rgb(255, 255, 255);cursor:pointer;display:inline-block;font-family:\"Helvetica Neue\", Arial, sans-serif;font-size:13px;font-stretch:100%;font-style:normal;font-variant-caps:normal;font-variant-east-asian:normal;font-variant-ligatures:normal;font-variant-numeric:normal;font-weight:500;height:28px;line-height:26px;outline-color:rgb(255, 255, 255);outline-style:none;outline-width:0px;padding-bottom:1px;padding-left:9px;padding-right:10px;padding-top:1px;position:relative;text-align:left;text-decoration-color:rgb(255, 255, 255);text-decoration-line:none;text-decoration-style:solid;text-decoration-thickness:auto;user-select:none;vertical-align:top;white-space:nowrap;zoom:1;'>\n",
        "                <i style='background-attachment:scroll;background-clip:border-box;background-color:rgba(0,0,0,0);background-image:url(data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%2072%2072%22%3E%3Cpath%20fill%3D%22none%22%20d%3D%22M0%200h72v72H0z%22%2F%3E%3Cpath%20class%3D%22icon%22%20fill%3D%22%23fff%22%20d%3D%22M68.812%2015.14c-2.348%201.04-4.87%201.744-7.52%202.06%202.704-1.62%204.78-4.186%205.757-7.243-2.53%201.5-5.33%202.592-8.314%203.176C56.35%2010.59%2052.948%209%2049.182%209c-7.23%200-13.092%205.86-13.092%2013.093%200%201.026.118%202.02.338%202.98C25.543%2024.527%2015.9%2019.318%209.44%2011.396c-1.125%201.936-1.77%204.184-1.77%206.58%200%204.543%202.312%208.552%205.824%2010.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163%200%206.345%204.513%2011.638%2010.504%2012.84-1.1.298-2.256.457-3.45.457-.845%200-1.666-.078-2.464-.23%201.667%205.2%206.5%208.985%2012.23%209.09-4.482%203.51-10.13%205.605-16.26%205.605-1.055%200-2.096-.06-3.122-.184%205.794%203.717%2012.676%205.882%2020.067%205.882%2024.083%200%2037.25-19.95%2037.25-37.25%200-.565-.013-1.133-.038-1.693%202.558-1.847%204.778-4.15%206.532-6.774z%22%2F%3E%3C%2Fsvg%3E);background-origin:padding-box;background-position-x:0px;background-position-y:0px;background-repeat-x;background-repeat-y;background-size:auto;color:rgb(255,255,255);cursor:pointer;display:inline-block;font-family:\"Helvetica Neue\",Arial,sans-serif;font-size:13px;font-stretch:100%;font-style:italic;font-variant-caps:normal;font-variant-east-asian:normal;font-variant-ligatures:normal;font-variant-numeric:normal;font-weight:500;height:18px;line-height:26px;position:relative;text-align:left;text-decoration-thickness:auto;top:4px;user-select:none;white-space:nowrap;width:18px;'></i>\n",
        "                <span style='color:rgb(255,255,255);cursor:pointer;display:inline-block;font-family:\"Helvetica Neue\",Arial,sans-serif;font-size:13px;font-stretch:100%;font-style:normal;font-variant-caps:normal;font-variant-east-asian:normal;font-variant-ligatures:normal;font-variant-numeric:normal;font-weight:500;line-height:26px;margin-left:4px;text-align:left;text-decoration-thickness:auto;user-select:none;vertical-align:top;white-space:nowrap;zoom:1;'>Tweet</span>\n",
        "            </a>\n",
        "            </div>\n",
        "            '''\n",
        "\n",
        "def share_model_table(artist_name, model_name):\n",
        "  url = f\"https://twitter.com/intent/tweet?text=I created an AI bot of {artist_name} with %23huggingartists!%0APlay with my model or create your own! &url=https://huggingface.co/huggingartists/{model_name}\"\n",
        "\n",
        "  share_button = get_share_button(url)\n",
        "  table_data = [\n",
        "        f'<tr><td>{share_button}</td><td>🎉 Share {artist_name} model: <a href=\"https://huggingface.co/huggingartists/{model_name}\">https://huggingface.co/huggingartists/{model_name}</a></td></tr>'    \n",
        "  ]\n",
        "  return get_table(table_data)\n",
        "\n",
        "def get_share_lyrics_url(artist_name, model_name, lyrics):\n",
        "   return \"https://twitter.com/intent/tweet?text=I created an AI bot of \" + artist_name + \" with %23huggingartists!%0A%0ABrand new song:%0A\" + lyrics.replace('\\n', '%0A').replace('\"', '%22') + \"%0A%0APlay with my model or create your own! &url=https://huggingface.co/huggingartists/\" + model_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "HdbKT-FpcP9c",
        "outputId": "d1a4fc04-6132-47fa-dc98-edc4f5171067"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-1822d1cade6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mkdir /root/.huggingface -p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtext_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/root/.huggingface/token\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtext_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mtext_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'token' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOwZFgODPC5a//9RHdfgkU2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}